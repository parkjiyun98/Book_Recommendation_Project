{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbee4dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"infoWrap_txt\">\n",
      "                        <div class=\"infoWrap_txtInner\">\n",
      "                            <textarea class=\"txtContentText\" style=\"display:none;\">&lt;b&gt;“그들은 서로 사랑했는데도 &lt;br/&gt;상대방에게 하나의 지옥을 선사했다.”&lt;br/&gt;&lt;br/&gt;국내 출간 30주년 및 국내 총 판매량 100만부 달성 기념 &lt;br/&gt;리뉴얼 단행본 출간&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;매해 노벨 문학상 후보 목록에 오르는 작가인 동시에 인터뷰나 대외 활동을 자제하고 은둔을 자처하는 작가. 체코 출신으로 ‘프라하의 봄’을 직접 경험하고 집필 및 판매 금지 등 정치적 박해를 피해 프랑스로 망명한 작가. 현재에서 멀지 않은 20세기 작가이지만 이미 살아 있는 신화가 된 작가. 밀란 쿤데라에 대한 한국인의 사랑은 특별하다. 대표작 『참을 수 없는 존재의 가벼움』의 국내 총 판매량 100만 부에 달하며, 민음사에서는 밀란 쿤데라 전집(총 15권)을 출간하기도 했다. 쿤데라를 사랑하는 독자는 광고인 박웅현, 피아니스트 김대진, 화가 황주리, 소설가 김영하, 김연수 등 각계각층에서 다양하다. 특히 지난 2016년에는 네이버 ‘지식인의 서재’ 선정 ‘우리 시대 지식인이 사랑한 책’ TOP10에 들기도 했다. 쿤데라에 대한 격찬은 그의 소설이 프랑스어로 소개된 직후 서양 지식인들에서부터 시작되었다. 쿤데라의 첫 번째 소설인 『농담』 불어판 서문에서 시인 아라공은 쿤데라를 일컬어 “금세기 최고의 소설가들 중 한 사람, 소설이 빵과 마찬가지로 인간에게 없어서는 안 되는 것임을 증명해 주는 작가”라고 격찬하며 “우리 시대 어떤 작가도 필적할 수 없는 기교를 갖추었다.”라고 했다. 또한 샐먼 루시디는 쿤데라를 “명백히 세계적으로 가장 훌륭한 예술가”라 칭했다.&lt;br/&gt;&lt;br/&gt;이렇듯 명실공히 20세기를 아울러 현존하는 최고의 현대 소설가 중 한 명으로 꼽히는 쿤데라의 작품들은 거의 모두가 탁월한 문학적 깊이를 인정받아서 프랑스 메디치 상, 클레멘트 루케 상, 프레미오 레테라리오 몬델로 상, 유로파 상, 체코 작가연맹 상, 체코 작가출판사 상, 커먼웰스 상, LA타임스 소설 상, 두카 재단 상 등 수많은 문학상을 받았으며 해마다 노벨 문학상 후보 작가로 추천되고 있다. 미국 미시건 대학은 그의 문학적 공로를 높이 평가하면서 명예박사 학위를 수여했다.&lt;br/&gt;&lt;br/&gt;쿤데라 작품을 독점 계약, 출판하고 있는 민음사에서는 밀란 쿤데라 국내 소개 30주년을 맞아 『참을 수 없는 존재의 가벼움』 리뉴얼 판을 선보였다. 『참을 수 없는 존재의 가벼움』은 1988년 계간 [세계의 문학] 가을호에 전재되면서 우리나라에 처음 소개되었고, 발표 직후 1988년 11월 20일 단행본으로 출간되었다. 당시에는 독문학자 송동준 교수가 독일어 판본을 옮겨 펴냈으나, 1999년 2월에 불문학자 이재룡 교수의 변역으로 다시 펴냈다. 이는 원저자인 밀란 쿤데라의 요청에 따른 것으로, 쿤데라는 프랑스어 판본을 옮기는 것이 자신의 원작에 가장 충실한 것이라고 밝힌 바 있다. 새롭게 리뉴얼해 선보이는 『참을 수 없는 존재의 가벼움』은 그간 출간된 세계문학전집 및 작가 전집 버전과 달리 밀란 쿤데라가 직접 그린 일러스트를 바탕으로 디자인한 신선한 표지와 장정으로 21세기를 살아 나가는 젊은 독자들의 눈을 다시금 사로잡을 예정이다.                    </textarea>\n",
      "                        </div>\n",
      "                    </div>]\n",
      "\n",
      "                        \n",
      "                            <b>“그들은 서로 사랑했는데도 <br/>상대방에게 하나의 지옥을 선사했다.”<br/><br/>국내 출간 30주년 및 국내 총 판매량 100만부 달성 기념 <br/>리뉴얼 단행본 출간</b><br/><br/>매해 노벨 문학상 후보 목록에 오르는 작가인 동시에 인터뷰나 대외 활동을 자제하고 은둔을 자처하는 작가. 체코 출신으로 ‘프라하의 봄’을 직접 경험하고 집필 및 판매 금지 등 정치적 박해를 피해 프랑스로 망명한 작가. 현재에서 멀지 않은 20세기 작가이지만 이미 살아 있는 신화가 된 작가. 밀란 쿤데라에 대한 한국인의 사랑은 특별하다. 대표작 『참을 수 없는 존재의 가벼움』의 국내 총 판매량 100만 부에 달하며, 민음사에서는 밀란 쿤데라 전집(총 15권)을 출간하기도 했다. 쿤데라를 사랑하는 독자는 광고인 박웅현, 피아니스트 김대진, 화가 황주리, 소설가 김영하, 김연수 등 각계각층에서 다양하다. 특히 지난 2016년에는 네이버 ‘지식인의 서재’ 선정 ‘우리 시대 지식인이 사랑한 책’ TOP10에 들기도 했다. 쿤데라에 대한 격찬은 그의 소설이 프랑스어로 소개된 직후 서양 지식인들에서부터 시작되었다. 쿤데라의 첫 번째 소설인 『농담』 불어판 서문에서 시인 아라공은 쿤데라를 일컬어 “금세기 최고의 소설가들 중 한 사람, 소설이 빵과 마찬가지로 인간에게 없어서는 안 되는 것임을 증명해 주는 작가”라고 격찬하며 “우리 시대 어떤 작가도 필적할 수 없는 기교를 갖추었다.”라고 했다. 또한 샐먼 루시디는 쿤데라를 “명백히 세계적으로 가장 훌륭한 예술가”라 칭했다.<br/><br/>이렇듯 명실공히 20세기를 아울러 현존하는 최고의 현대 소설가 중 한 명으로 꼽히는 쿤데라의 작품들은 거의 모두가 탁월한 문학적 깊이를 인정받아서 프랑스 메디치 상, 클레멘트 루케 상, 프레미오 레테라리오 몬델로 상, 유로파 상, 체코 작가연맹 상, 체코 작가출판사 상, 커먼웰스 상, LA타임스 소설 상, 두카 재단 상 등 수많은 문학상을 받았으며 해마다 노벨 문학상 후보 작가로 추천되고 있다. 미국 미시건 대학은 그의 문학적 공로를 높이 평가하면서 명예박사 학위를 수여했다.<br/><br/>쿤데라 작품을 독점 계약, 출판하고 있는 민음사에서는 밀란 쿤데라 국내 소개 30주년을 맞아 『참을 수 없는 존재의 가벼움』 리뉴얼 판을 선보였다. 『참을 수 없는 존재의 가벼움』은 1988년 계간 [세계의 문학] 가을호에 전재되면서 우리나라에 처음 소개되었고, 발표 직후 1988년 11월 20일 단행본으로 출간되었다. 당시에는 독문학자 송동준 교수가 독일어 판본을 옮겨 펴냈으나, 1999년 2월에 불문학자 이재룡 교수의 변역으로 다시 펴냈다. 이는 원저자인 밀란 쿤데라의 요청에 따른 것으로, 쿤데라는 프랑스어 판본을 옮기는 것이 자신의 원작에 가장 충실한 것이라고 밝힌 바 있다. 새롭게 리뉴얼해 선보이는 『참을 수 없는 존재의 가벼움』은 그간 출간된 세계문학전집 및 작가 전집 버전과 달리 밀란 쿤데라가 직접 그린 일러스트를 바탕으로 디자인한 신선한 표지와 장정으로 21세기를 살아 나가는 젊은 독자들의 눈을 다시금 사로잡을 예정이다.                    \n",
      "                        \n",
      "                    \n",
      "[('쿤데라', 13), ('작가', 11), ('문학', 9), ('소설', 7), ('밀란', 5), ('세기', 4), ('사랑', 4), ('프랑스', 4), ('국내', 4), ('문학상', 3), ('판매', 3), ('지식인', 3), ('소설가', 3), ('체코', 3), ('소개', 3), ('리뉴', 3), ('세계', 3), ('전집', 3), ('리뉴얼', 3), ('최고', 2), ('직접', 2), ('대한', 2), ('단행본', 2), ('작품', 2), ('후보', 2), ('출판', 2), ('주년', 2), ('교수', 2), ('화가', 2), ('판매량', 2), ('프랑스어', 2), ('노벨', 2), ('판본', 2), ('민음사', 2), ('독자', 2), ('직후', 2), ('시대', 2), ('기도', 2), ('격찬', 2), ('대표', 1), ('원작', 1), ('당시', 1), ('피해', 1), ('깊이', 1), ('은둔', 1), ('예술가', 1), ('활동', 1), ('미국', 1), ('바탕', 1), ('자제', 1), ('금세기', 1), ('광고인', 1), ('가을', 1), ('서양', 1), ('메디치', 1), ('지난', 1), ('계간', 1), ('시인', 1), ('다양하다', 1), ('특별하다', 1), ('선사', 1), ('피아니스트', 1), ('서재', 1), ('발표', 1), ('김대진', 1), ('현재', 1), ('현대', 1), ('루시', 1), ('한국인', 1), ('유로파', 1), ('명실', 1), ('클레', 1), ('출판사', 1), ('김연수', 1), ('박해', 1), ('김영하', 1), ('아라공', 1), ('그린', 1), ('경험', 1), ('추천', 1), ('마찬가지', 1), ('미시건', 1), ('표지', 1), ('연맹', 1), ('요청', 1), ('타임스', 1), ('박사', 1), ('인간', 1), ('재단', 1), ('계약', 1), ('공로', 1), ('독점', 1), ('네이버', 1), ('다시금', 1), ('필적', 1), ('장정', 1), ('예정', 1), ('신화', 1), ('불문학', 1), ('금지', 1), ('각계', 1), ('그간', 1), ('우리나라', 1), ('대학', 1), ('프레', 1), ('리오', 1), ('이재룡', 1), ('서문', 1), ('망명', 1), ('정치', 1), ('자처', 1), ('목록', 1), ('루케', 1), ('박웅', 1), ('버전', 1), ('몬델', 1), ('대외', 1), ('기교', 1), ('높이', 1), ('변역', 1), ('저자', 1), ('인터뷰', 1), ('멘트', 1), ('독문학', 1), ('독일어', 1), ('동준', 1), ('증명', 1), ('레테', 1), ('지옥', 1), ('달성', 1), ('일러스트', 1), ('수여', 1), ('농담', 1), ('프라하', 1), ('디자인', 1), ('서로', 1), ('선정', 1), ('기념', 1), ('상대방', 1), ('주리', 1), ('평가', 1), ('출신', 1), ('커먼웰스', 1), ('명예', 1), ('집필', 1), ('각층', 1), ('학위', 1), ('처음', 1), ('세계문학', 1), ('칭했다', 1), ('달리', 1), ('밉다', 0), ('수많다', 0), ('훌륭하다', 0), ('탁월하다', 0), ('이렇다', 0), ('신선하다', 0), ('가볍다', 0), ('젊다', 0), ('새롭다', 0), ('멀다', 0), ('어떻다', 0), ('명백하다', 0), ('충실하다', 0)]\n",
      "['쿤데라', '작가', '문학', '소설', '밀란', '세기', '사랑', '프랑스', '국내', '문학상', '판매', '지식인', '소설가', '체코', '소개', '리뉴', '세계', '전집', '리뉴얼', '최고', '직접', '대한', '단행본', '작품', '후보', '출판', '주년', '교수', '화가', '판매량', '프랑스어', '노벨', '판본', '민음사', '독자', '직후', '시대', '기도', '격찬', '대표', '원작', '당시', '피해', '깊이', '은둔', '예술가', '활동', '미국', '바탕', '자제', '금세기', '광고인', '가을', '서양', '메디치', '지난', '계간', '시인', '다양하다', '특별하다', '선사', '피아니스트', '서재', '발표', '김대진', '현재', '현대', '루시', '한국인', '유로파', '명실', '클레', '출판사', '김연수', '박해', '김영하', '아라공', '그린', '경험', '추천', '마찬가지', '미시건', '표지', '연맹', '요청', '타임스', '박사', '인간', '재단', '계약', '공로', '독점', '네이버', '다시금', '필적', '장정', '예정', '신화', '불문학', '금지', '각계', '그간', '우리나라', '대학', '프레', '리오', '이재룡', '서문', '망명', '정치', '자처', '목록', '루케', '박웅', '버전', '몬델', '대외', '기교', '높이', '변역', '저자', '인터뷰', '멘트', '독문학', '독일어', '동준', '증명', '레테', '지옥', '달성', '일러스트', '수여', '농담', '프라하', '디자인', '서로', '선정', '기념', '상대방', '주리', '평가', '출신', '커먼웰스', '명예', '집필', '각층', '학위', '처음', '세계문학', '칭했다', '달리', '밉다', '수많다', '훌륭하다', '탁월하다', '이렇다', '신선하다', '가볍다', '젊다', '새롭다', '멀다', '어떻다', '명백하다', '충실하다']\n",
      "['쿤데라', '작가', '문학', '소설', '밀란']\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from konlpy.tag import Kkma\n",
    "# from konlpy.utils import pprint\n",
    "\n",
    "# #sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding= 'utf-8')\n",
    "# #sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding = 'utf-8')\n",
    "# kkma = Kkma()\n",
    "\n",
    "# url = 'http://www.yes24.com/Product/Goods/61933303'\n",
    "# res = requests.post(url)\n",
    "# soup = BeautifulSoup(res.text, 'html5lib')\n",
    "# tag_name = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > div > h2'\n",
    "# tag_description = '#infoset_introduce > div.infoSetCont_wrap > div.infoWrap_txt'\n",
    "# #tag_description = '#infoset_introduce > div.infoSetCont_wrap > div > div'\n",
    "\n",
    "# try:\n",
    "#     name = sub_soup.select(tag_name)[0].text\n",
    "# except:      \n",
    "#     name = '-'\n",
    "\n",
    "# name = soup.select(tag_name)[0].text\n",
    "# books = soup.select(tag_description)\n",
    "# print(books)\n",
    "\n",
    "# description = books[0].get_text()\n",
    "# print(description)\n",
    "\n",
    "# #koNoun = (kkma.nouns(description))\n",
    "# #print(koNoun)\n",
    "# characters = \"<b></br>/</b><br>B\"\n",
    "# description = ''.join( x for x in description if x not in characters)\n",
    "                \n",
    "# stopwords = '아 어제 가장 거기 먼저 보통 이전 이후 그동안 오늘날 지금 파의 편의 가의 가인 마주 역사 비롯 것임 많다 일인 위함 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓'\n",
    "\n",
    "#                 # 형태소 분석 ( norm : 정규화 ) /형태소 단위로 쪼개고 각 품사를 태깅해서 리스트형태로 반한   \n",
    "# okt_pos = Okt().pos(description, norm=True, stem=True)\n",
    "#                 #품사가 명사와 형용사일 때, 단어길이가 2이상일 때 추출\n",
    "# okt_filtering = [x for x, y in okt_pos if y in ['Noun', 'Adjective'] if len(x) >= 2 if x not in stopwords if x not in name]\n",
    "\n",
    "#     #             kkma_pos = Kkma().pos(description)\n",
    "#     #             kkma_filtering = [x for x, y in kkma_pos if y in ['NNG', 'NNP', 'VA'] if len(x) >= 2 if x not in stopwords if x not in name]\n",
    "#     #             tokenizer = \" \".join(kkma_filtering)\n",
    "#     #             count_dict = [(token, description.count(token)) for token in kkma_filtering ]\n",
    "\n",
    "\n",
    "# tokenizer = \" \".join(okt_filtering)\n",
    "# count_dict = [(token, description.count(token)) for token in okt_filtering ]\n",
    "# #print(count_dict)\n",
    "# ranked_words = sorted(count_dict, key=lambda x:x[1], reverse=True)\n",
    "# ranked_words = set(ranked_words)\n",
    "# ranked_words = sorted(ranked_words, key = lambda x: x[1], reverse=True)\n",
    "# print(ranked_words)\n",
    "# keyword = [ keyword for keyword, freq in ranked_words ]\n",
    "# print(keyword)\n",
    "# five_keyword = keyword[:5]\n",
    "# print(five_keyword)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f4b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"infoWrap_txt\">\n",
      "                        <div class=\"infoWrap_txtInner\">\n",
      "                            <textarea class=\"txtContentText\" style=\"display:none;\">&lt;b&gt;“세월이 이따금 묻는다/ 사랑은 그 후 어떻게 되었느냐고”&lt;br/&gt;&lt;br/&gt;[그대가 곁에 있어도 나는 그대가 그립다]에 이은&lt;br/&gt;류시화 시인의 두 번째 시집&lt;br/&gt;&lt;/b&gt;&lt;br/&gt;1997년에 출간한 류시화 시인의 두 번째 시집 개정판이다. 시인이 서문에 썼듯이 초판본에 실었던 시들 중에서 여러 편을 수정하고, '마음에 들지 않는 시'들을 덜어냈다. 그러나 시 속에 담긴 시인의 시선은 변함이 없다. 아무리 해도 '버려지지 않는 것들'이 있기 때문일 것이다. 시인은 쓸쓸한 언어로 화사한 인생을 노래해야 하는 운명을 지닌 사람이다. 그래서 시는 이상하다. '삶에는 시로써만 말할 수 있는 것'이 있으며, '사물들은 시인을 통해 말하고 싶어 한다'는 것이 류시화의 일관된 시정신이다. 여기, 특별한 시적 감각과 삶을 응시하는 독특한 시선, 그리고 이 세계를 마주한 사유가 돋보이는 48편의 시가 있다. 가수 안치환이 노래로 부른 [소금인형]처럼 류시화의 시는 소리 내어 읽을 때 더 울림이 크다.&lt;br/&gt;&lt;br/&gt;                    </textarea>\n",
      "                        </div>\n",
      "                    </div>]\n",
      "\n",
      "                        \n",
      "                            <b>“세월이 이따금 묻는다/ 사랑은 그 후 어떻게 되었느냐고”<br/><br/>[그대가 곁에 있어도 나는 그대가 그립다]에 이은<br/>류시화 시인의 두 번째 시집<br/></b><br/>1997년에 출간한 류시화 시인의 두 번째 시집 개정판이다. 시인이 서문에 썼듯이 초판본에 실었던 시들 중에서 여러 편을 수정하고, '마음에 들지 않는 시'들을 덜어냈다. 그러나 시 속에 담긴 시인의 시선은 변함이 없다. 아무리 해도 '버려지지 않는 것들'이 있기 때문일 것이다. 시인은 쓸쓸한 언어로 화사한 인생을 노래해야 하는 운명을 지닌 사람이다. 그래서 시는 이상하다. '삶에는 시로써만 말할 수 있는 것'이 있으며, '사물들은 시인을 통해 말하고 싶어 한다'는 것이 류시화의 일관된 시정신이다. 여기, 특별한 시적 감각과 삶을 응시하는 독특한 시선, 그리고 이 세계를 마주한 사유가 돋보이는 48편의 시가 있다. 가수 안치환이 노래로 부른 [소금인형]처럼 류시화의 시는 소리 내어 읽을 때 더 울림이 크다.<br/><br/>                    \n",
      "                        \n",
      "                    \n",
      "세월 사랑 어떻다 그대 있다 그대 류시화 시인 시집 류시화 시인 시집 개정판 시인 서문 초판 실었던 여러 수정 마음 시인 시선 변하다 없다 해도 있다 때문 시인 쓸쓸하다 언어 화사하다 인생 노래 운명 사람 이상하다 로써 있다 있다 사물 시인 통해 류시화 일관 정신 여기 특별하다 시적 감각 응시 독특하다 시선 세계 마주 사유 편의 시가 있다 가수 안치환 노래 소금인형 처럼 류시화 소리 울림\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import sqlite3\n",
    "\n",
    "# url = 'http://www.yes24.com/Product/Goods/26932917'\n",
    "# res = requests.post(url)\n",
    "# soup = BeautifulSoup(res.text, 'html5lib')\n",
    "# tag_description = '#infoset_introduce > div.infoSetCont_wrap > div.infoWrap_txt'\n",
    "\n",
    "# books = soup.select(tag_description)\n",
    "# print(books)\n",
    "\n",
    "# description = books[0].get_text()\n",
    "# print(description)\n",
    "\n",
    "# #데이터안에 명사 형용사만 뽑아내기\n",
    "# from konlpy.tag import Okt\n",
    "\n",
    "# okt_pos = Okt().pos(description, norm=True, stem=True)   # 형태소 분석 ( norm : 정규화 )\n",
    "\n",
    "\n",
    "# okt_filtering = [x for x, y in okt_pos if y in ['Noun', 'Adjective'] if len(x) >= 2]\n",
    "# tokenizer = \" \".join(okt_filtering)\n",
    "# print(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9781a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "책제목#yDetailTopWrap > div.topColRgt > div.gd_infoTop > div > h2\n",
    "책소개#infoset_introduce > div.infoSetCont_wrap > div.infoWrap_txt > div\n",
    "##infoset_introduce > div.infoSetCont_wrap > div > div\n",
    "책저자#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_auth > a\n",
    "책저자2#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_pub > a\n",
    "#infoset_introduce > div.infoSetCont_wrap\n",
    "#infoset_introduce > div.infoSetCont_wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dbc63d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from konlpy.utils import pprint\n",
    "import sqlite3\n",
    "\n",
    "# 데이터 수집 함수 정의\n",
    "def yes24DataReader(CategoryNumber, year, month):\n",
    "\n",
    "    root_url = 'http://www.yes24.com'\n",
    "\n",
    "    url_1 = 'http://www.yes24.com/24/category/bestseller?CategoryNumber='\n",
    "    url_2 = '&sumgb=09&year='\n",
    "    url_3 = '&month='\n",
    "    url_4 = '&PageNumber='\n",
    "    url_set = url_1 + CategoryNumber + url_2 + year + url_3 + month + url_4\n",
    "\n",
    "    book_list=[]\n",
    "    \n",
    "\n",
    "    # 월 별 조회 시 최대 50쪽이지만, 간단하게 2쪽까지만 수집\n",
    "    for i in range(1, 32):\n",
    "\n",
    "        url = url_set + str(i)\n",
    "\n",
    "        res = requests.post(url)\n",
    "        soup = BeautifulSoup(res.text, 'html5lib')\n",
    "        tag = '#category_layout > tbody > tr > td.goodsTxtInfo > p:nth-of-type(1) > a:nth-of-type(1)'\n",
    "        books = soup.select(tag)\n",
    "\n",
    "        # 수집 중인 페이지 번호 출력\n",
    "        print('# Page', i)\n",
    "        \n",
    "        # 개별 도서 정보 수집\n",
    "        for book in books:\n",
    "            index = 0\n",
    "\n",
    "            sub_url = root_url + book.attrs['href']\n",
    "            sub_res = requests.post(sub_url)\n",
    "            sub_soup = BeautifulSoup(sub_res.text, 'html5lib')\n",
    "\n",
    "            tag_name = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > div > h2'\n",
    "            tag_author = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_auth > a'\n",
    "            tag_author2 = '#yDetailTopWrap > div.topColRgt > div.gd_infoTop > span.gd_pubArea > span.gd_auth'\n",
    "            tag_description = '#infoset_introduce > div.infoSetCont_wrap > div.infoWrap_txt'\n",
    "            #infoset_introduce > div.infoSetCont_wrap > div > div\n",
    "            #infoset_introduce > div.infoSetCont_wrap > div.infoWrap_txt > div\n",
    "\n",
    "            # 기본적인 예외처리를 통한 데이터 수집\n",
    "            name = sub_soup.select(tag_name)[0].text\n",
    "            \n",
    "            try:\n",
    "                author = sub_soup.select(tag_author)[0].text\n",
    "            except:\n",
    "                author = sub_soup.select(tag_author2)[0].text.strip('\\n').strip().replace(' 저','')\n",
    "\n",
    "            try:\n",
    "                description = sub_soup.select(tag_description)[0].text\n",
    "            except:\n",
    "                description = '-'\n",
    "                \n",
    "            characters = \"<b></br>/</b><br>B\"\n",
    "            description = ''.join( x for x in description if x not in characters)\n",
    "                \n",
    "#             stopwords = '아 어제 가장 거기 먼저 독자 보통 작가 소설 작품 이전 이후 그동안 오늘날 지금 파의 편의 가의 가인 마주 역사 비롯 것임 많다 일인 위함 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓'\n",
    "            \n",
    "#             # 형태소 분석 ( norm : 정규화 ) /형태소 단위로 쪼개고 각 품사를 태깅해서 리스트형태로 반한   \n",
    "#             okt_pos = Okt().pos(description, norm=True, stem=True)\n",
    "#             #품사가 명사와 형용사일 때, 단어길이가 2이상일 때 추출\n",
    "#             okt_filtering = [x for x, y in okt_pos if y in ['Noun', 'Adjective'] if len(x) >= 2 if x not in stopwords if x not in name]\n",
    "            \n",
    "# #             kkma_pos = Kkma().pos(description)\n",
    "# #             kkma_filtering = [x for x, y in kkma_pos if y in ['NNG', 'NNP', 'VA'] if len(x) >= 2 if x not in stopwords if x not in name]\n",
    "# #             tokenizer = \" \".join(kkma_filtering)\n",
    "# #             count_dict = [(token, description.count(token)) for token in kkma_filtering ]\n",
    "            \n",
    "            \n",
    "#             tokenizer = \" \".join(okt_filtering)\n",
    "#             count_dict = [(token, description.count(token)) for token in okt_filtering ]\n",
    "#             #print(count_dict)\n",
    "            \n",
    "            \n",
    "#             ranked_words = sorted(count_dict, key=lambda x:x[1], reverse=True)\n",
    "#             ranked_words = set(ranked_words)\n",
    "#             ranked_words = sorted(ranked_words, key = lambda x: x[1], reverse=True)\n",
    "#             keyword = [ keyword for keyword, freq in ranked_words ]\n",
    "#             five_keyword = keyword[:5]\n",
    "#             #five_keyword = \" \".join(five_keyword)\n",
    "            \n",
    "            \n",
    "# #             if description != '-':\n",
    "# # #       \n",
    "# #                 book_list.append([name, author, description, tokenizer, five_keyword])\n",
    "                \n",
    "# #             if description != '-':\n",
    "# #                 book_list.append([name, author, description])\n",
    "            \n",
    "# #             if description != '-':\n",
    "# #                  book_list.append([name, five_keyword])\n",
    "            textrank = TextRank(description)\n",
    "            five_keyword = textrank.keywords()\n",
    "\n",
    "#             if description != '-' and name != '-':\n",
    "#                   book_list.append([name, five_keyword, five_keyword[0],five_keyword[1], five_keyword[2], five_keyword[3], five_keyword[4]])\n",
    "            \n",
    "            if description != '-':\n",
    "                book_list.append([name, author, description, tokenizer, five_keyword])\n",
    "            print('=========>', name)\n",
    "\n",
    "    # 데이터프레임 컬럼명 지정\n",
    "    colList = ['name',  'author','description', 'tokenizer', 'five_keyword']\n",
    "    #colList = ['name','five_keyword']\n",
    "    \n",
    "    #colList = [ 'name', 'five_keyword','five_keyword1','five_keyword2','five_keyword3','five_keyword4','five_keyword5']\n",
    "\n",
    "    # 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(np.array(book_list), columns=colList)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88881455",
   "metadata": {},
   "outputs": [],
   "source": [
    " from newspaper import Article\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "        self.okt = Okt()\n",
    "        #self.stopwords = pd.read_csv('Korean_Stopwords')\n",
    "        stopwords = '아 어제 가장 오늘날 먼저 보통 오늘날 본때 지금 파의 편의 가의 가인 마주 역사 비롯 것임 많다 일인 위함 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓'\n",
    "        self.stopwords = stopwords.split() \n",
    "  \n",
    "    def text2sentences(self, text):\n",
    "        sentences = self.kkma.sentences(text)      \n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        \n",
    "        return sentences\n",
    "\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence != '':\n",
    "                nouns.append(' '.join([noun for noun in self.okt.nouns(str(sentence)) \n",
    "                                       if noun not in self.stopwords and len(noun) > 1]))\n",
    "        \n",
    "        return nouns\n",
    " \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return  self.graph_sentence\n",
    "        \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로 \n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "            \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        \n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        \n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "                    \n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        \n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "        self.word_rank_idx =  self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "        \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        \n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    def keywords(self, word_num=5):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        \n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        \n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "14c5a018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# Year 2019\n",
      "==================================================\n",
      "==================================================\n",
      "# Month 9\n",
      "==================================================\n",
      "# Page 1\n",
      "=========> 여행의 이유\n",
      "=========> 튜브, 힘낼지 말지는 내가 결정해\n",
      "=========> 나는 나로 살기로 했다 (100만 부 기념 클래식 에디션)\n",
      "=========> 제 인생에 답이 없어요\n",
      "=========> 언어의 온도\n",
      "=========> 잊기 좋은 이름\n",
      "=========> 고요할수록 밝아지는 것들\n",
      "=========> 걷는 사람, 하정우\n",
      "=========> 시절일기\n",
      "=========> 박막례, 이대로 죽을 순 없다\n",
      "=========> 오늘은 이만 좀 쉴게요 \n",
      "=========> 마음의 결\n",
      "=========> 인생의 마지막 순간에서\n",
      "=========> 축구를 하며 생각한 것들\n",
      "=========> 나는 아기 캐리어가 아닙니다\n",
      "=========> 곰돌이 푸, 행복한 일은 매일 있어 \n",
      "=========> 쓸 만한 인간\n",
      "=========> 하마터면 열심히 살 뻔했다\n",
      "=========> 우리 이만 헤어져요\n",
      "=========> 100세 철학자의 인생, 희망 이야기\n",
      "# Page 2\n",
      "=========> 무례한 사람에게 웃으며 대처하는 법\n",
      "=========> 내가 확실히 아는 것들\n",
      "=========> 마음사전\n",
      "=========> 서른이면 어른이 될 줄 알았다\n",
      "=========> 어피치, 마음에도 엉덩이가 필요해\n",
      "=========> 라이언, 내 곁에 있어줘\n",
      "=========> 이런 사람 만나지 마세요\n",
      "=========> 좋은지 나쁜지 누가 아는가\n",
      "=========> 죽고 싶지만 떡볶이는 먹고 싶어\n",
      "=========> 100세 철학자의 철학, 사랑 이야기\n",
      "=========> 참 소중한 너라서\n",
      "=========> 운다고 달라지는 일은 아무것도 없겠지만\n",
      "=========> 어른답게 삽시다\n",
      "=========> 내가 모르는 것이 참 많다\n",
      "=========> 골든아워 1\n",
      "=========> 내 인생의 해답\n",
      "=========> 일본산고\n",
      "=========> 나는 어른이 되어서도 가끔 울었다\n",
      "=========> 평범히 살고 싶어 열심히 살고 있다 (핑크 에디션)\n",
      "=========> 골든아워 2\n",
      "# Page 3\n",
      "=========> 죽음의 수용소에서\n",
      "=========> 반 고흐, 영혼의 편지\n",
      "=========> 곰돌이 푸, 서두르지 않아도 괜찮아\n",
      "=========> 여자 둘이 살고 있습니다 (서사음 에디션)\n",
      "=========> 내가 죽으면 장례식에 누가 와줄까\n",
      "=========> 모든 순간이 너였다\n",
      "=========> 힘 좀 빼고 삽시다\n",
      "=========> 죽고 싶지만 떡볶이는 먹고 싶어 2\n",
      "=========> 나를 상하게 하는 일은 그만하기로 했다\n",
      "=========> 아무튼, 문구\n",
      "=========> 최소한에서 최대한으로 시현하다\n",
      "=========> 요리는 감이여\n",
      "=========> 오늘의 퀴즈\n",
      "=========> 미친 사랑의 서\n",
      "=========> 사랑에는 사랑이 없다\n",
      "=========> 타인의 시선을 의식해 힘든 나에게\n",
      "=========> 나는 죽을 때까지 재미있게 살고 싶다\n",
      "=========> 연애 백문백답\n",
      "=========> 연필로 쓰기\n",
      "=========> 숨결이 바람 될 때\n",
      "# Page 4\n",
      "=========> 아침의 피아노\n",
      "=========> 멈추면, 비로소 보이는 것들\n",
      "=========> 참 애썼다 그것으로 되었다\n",
      "=========> 모리와 함께한 화요일\n",
      "=========> 아이가 잠들면 서재로 숨었다\n",
      "=========> 당신의 마음을 안아줄게요\n",
      "=========> 있으려나 서점\n",
      "=========> 윤동주 전 시집\n",
      "=========> 태도에 관하여\n",
      "=========> 감옥으로부터의 사색\n",
      "=========> 이별의 푸가\n",
      "=========> 나는 내 파이를 구할 뿐 인류를 구하러 온 게 아니라고\n",
      "=========> 월든\n",
      "=========> 땅끄부부, 무모하지만 결국엔 참 잘한 일\n",
      "=========> 완벽하지 않은 것들에 대한 사랑\n",
      "=========> 불안\n",
      "=========> 인연\n",
      "=========> 노견일기 1\n",
      "=========> 너라는 계절\n",
      "=========> 아무튼, 술\n",
      "# Page 5\n",
      "=========> 인생은 이상하게 흐른다\n",
      "=========> 사실, 내성적인 사람입니다\n",
      "=========> 지금 이 순간을 살아라\n",
      "=========> 내가 가는 길이 꽃길이다\n",
      "=========> 게으른 게 아니라 충전 중입니다\n",
      "=========> 쉬운 일은 아니지만\n",
      "=========> 빨강머리 앤이 하는 말\n",
      "=========> 삶으로 다시 떠오르기\n",
      "=========> 위즈덤\n",
      "=========> 진짜 모습을 보이면 더는 사랑받지 못할까 봐 두려운 나에게\n",
      "=========> 삶의 쉼표가 필요할 때\n",
      "=========> 시를 잊은 그대에게 (리커버 에디션)\n",
      "=========> 100 인생 그림책\n",
      "=========> 나의 두 번째 이름, 허수아비\n",
      "=========> 인생수업\n",
      "=========> 새는 날아가면서 뒤돌아보지 않는다\n",
      "=========> 밤이 선생이다\n",
      "=========> 슬픔을 공부하는 슬픔\n",
      "=========> 일간 이슬아 수필집\n",
      "=========> 인생 수업\n",
      "# Page 6\n",
      "=========> 행복이 거기 있다, 한 점 의심도 없이\n",
      "=========> 타인을 안아주듯 나를 안았다\n",
      "=========> 반 고흐, 영혼의 편지 2\n",
      "=========> 감정수업\n",
      "=========> 바르셀로나 Barcelona\n",
      "=========> 내가 너를 사랑한 시간 내가 너를 사랑할 시간\n",
      "=========> 태도의 말들\n",
      "=========> 살아갈 날들을 위한 공부\n",
      "=========> 약간의 거리를 둔다\n",
      "=========> 사랑의 기술\n",
      "=========> 요즘 남자는 그렇지 않습니다\n",
      "=========> 당신이 허락한다면 나는 이 말 하고 싶어요\n",
      "=========> [예스리커버] 알지 못하는 아이의 죽음\n",
      "=========> 역마\n",
      "=========> 인생에서 너무 늦은 때란 없습니다\n",
      "=========> 잃었지만 잊지 않은 것들\n",
      "=========> 뼈 있는 아무 말 대잔치\n",
      "=========> 썅년의 미학\n",
      "=========> 장수 고양이의 비밀\n",
      "=========> 나는 프랑스 샤토에 산다\n",
      "# Page 7\n",
      "=========> 황현산의 사소한 부탁\n",
      "=========> 소소한 근육과 슬기로운 식사가 필요합니다\n",
      "=========> 환타지 없는 여행\n",
      "=========> 완벽하게 사랑하는 너에게\n",
      "=========> 우리가 보낸 가장 긴 밤\n",
      "=========> 살아온 기적 살아갈 기적\n",
      "=========> 하고 싶은 대로 살아도 괜찮아\n",
      "=========> 마녀체력\n",
      "=========> 나는 간호사, 사람입니다\n",
      "=========> 백악관 속기사는 핑크 슈즈를 신는다\n",
      "=========> 법에도 심장이 있다면\n",
      "=========> 마흔에 관하여\n",
      "=========> 스물아홉 생일, 1년 후 죽기로 결심했다\n",
      "=========> 우아하고 호쾌한 여자 축구\n",
      "=========> 김영하 산문 세트 \n",
      "=========> 온 마음을 다해 디저트\n",
      "=========> 나는 하버드에서도 책을 읽습니다\n",
      "=========> 사랑하게 해줘서, 고마워\n",
      "=========> 유배지에서 보낸 편지\n",
      "=========> 뿌리가 튼튼한 사람이 되고 싶어\n",
      "# Page 8\n",
      "=========> 잠시 고양이면 좋겠어\n",
      "=========> 내가 본 것을 당신도 볼 수 있다면\n",
      "=========> 바보 빅터\n",
      "=========> 마음이 처음 만들어졌을 때부터\n",
      "=========> 보노보노처럼 살다니 다행이야\n",
      "=========> 아무튼, 비건\n",
      "=========> 제가 결혼을 안 하겠다는 게 아니라\n",
      "=========> 나이팅게일은 죽었다\n",
      "=========> 용기를 잃지 말고 힘내요\n",
      "=========> 청춘의 문장들\n",
      "=========> 답장이 없으면 슬프긴 하겠다\n",
      "=========> 썅년의 미학, 플러스\n",
      "=========> 내 인생이다 임마\n",
      "=========> 빈센트 나의 빈센트\n",
      "=========> 나를 위해 하다\n",
      "=========> 아무것도 아닌 지금은 없다\n",
      "=========> 호찌냥찌\n",
      "=========> 스물다섯, 서른, 세계여행\n",
      "=========> 노력이라 쓰고 버티기라 읽는\n",
      "=========> 쓸 만한 인간\n",
      "# Page 9\n",
      "=========> 정원가의 열두 달\n",
      "=========> 무리하지 않는 선에서\n",
      "=========> 사랑의 기술\n",
      "=========> 혜민 스님의 따뜻한 응원\n",
      "=========> 키키 키린\n",
      "=========> 그쪽의 풍경은 환한가\n",
      "=========> 그놈의 소속감\n",
      "=========> 마음아, 넌 누구니\n",
      "=========> 보통의 존재\n",
      "=========> 광고천재 이제석\n",
      "=========> 헬렌 켈러 자서전\n",
      "=========> 자기만의 방\n",
      "=========> 혼자 살면 어때요? 좋으면 그만이지\n",
      "=========> 깃털 도둑\n",
      "=========> 가끔 여행하고 매일 이사합니다\n",
      "=========> 가볍지만 가볍지 않은\n",
      "=========> 행복해지는 연습을 해요\n",
      "=========> 다정한 구원\n",
      "=========> 좋아하는 게 너무 많아도 좋아\n",
      "=========> 나는 가해자의 엄마입니다\n",
      "# Page 10\n",
      "=========> 팔순에 한글 공부를 시작했습니다\n",
      "=========> 나, 있는 그대로 참 좋다\n",
      "=========> 술 취한 코끼리 길들이기\n",
      "=========> 딸에게 주는 레시피\n",
      "=========> 나만 두려운 건 아니겠지?\n",
      "=========> 힘 빼기의 기술\n",
      "=========> 달리기를 말할 때 내가 하고 싶은 이야기\n",
      "=========> 찰리 브라운, 걱정이 없으면 걱정이 없겠네\n",
      "=========> 저 청소일 하는데요?\n",
      "=========> 발레가 내 삶도 한 뼘 키워줄까요?\n",
      "=========> 베르나르 베르베르의 상상력 사전\n",
      "=========> 일단 오늘은 나한테 잘합시다\n",
      "=========> 스누피, 나도 내가 참 좋은걸\n",
      "=========> 어느 소방관의 기도\n",
      "=========> 회사 체질이 아니라서요\n",
      "=========> 구체적 사랑\n",
      "=========> 언제 들어도 좋은 말\n",
      "=========> 미안하지만, 오늘은 내 인생이 먼저예요\n",
      "=========> 한때 소중했던 것들 (볕뉘 에디션)\n",
      "=========> 단 하루도 너를 사랑하지 않은 날이 없다\n",
      "# Page 11\n",
      "=========> 광대하고 게으르게\n",
      "=========> 나는 심리치료사입니다\n",
      "=========> 싹싹하진 않아도 충분히 잘 하고 있습니다\n",
      "=========> 숲을 닮은 너에게\n",
      "=========> 그리고 모든 것이 변했다\n",
      "=========> 이것이 인간인가\n",
      "=========> 아무튼, 요가\n",
      "=========> 지구별 여행자\n",
      "=========> 자전거여행\n",
      "=========> 너에게 여름을 보낸다\n",
      "=========> 요가 좀 합니다\n",
      "=========> 나의 까만 단발머리\n",
      "=========> 신과 나눈 이야기 1\n",
      "=========> 우리가 글을 몰랐지 인생을 몰랐나\n",
      "=========> 만약은 없다\n",
      "=========> 따뜻한 밥이 되는 꿈\n",
      "=========> 어차피 일할 거라면, Porto\n",
      "=========> 학문의 즐거움\n",
      "=========> NU’EST ROAD 우리가 가는 길\n",
      "=========> 모든 요일의 여행 (10만 부 기념 리커버 에디션)\n",
      "# Page 12\n",
      "=========> 그래도 괜찮은 하루\n",
      "=========> HEAL THE WORLD 힐 더 월드\n",
      "=========> 시련은 있어도 실패는 없다\n",
      "=========> 솔직함의 적정선\n",
      "=========> 너의 모든 순간을 사랑해 Love Diary\n",
      "=========> 나는 왜 너가 아니고 나인가\n",
      "=========> 나에게 고맙다\n",
      "=========> 소란\n",
      "=========> 세상에서 가장 느린 달팽이의 속도로\n",
      "=========> 싸울 때마다 투명해진다\n",
      "=========> 바람이 분다 당신이 좋다\n",
      "=========> 우리 이제 낭만을 이야기합시다\n",
      "=========> 말하다\n",
      "=========> 열심히 사는 게 뭐가 어때서\n",
      "=========> 간호사라서 다행이야\n",
      "=========> 김이나의 작사법\n",
      "=========> 내가 사랑스럽지 않은 날에\n",
      "=========> 모든 것은 그 자리에\n",
      "=========> 떠나지 않으면 우린 영원히 몰라\n",
      "=========> 나는 꿈꾸는 간호사입니다\n",
      "# Page 13\n",
      "=========> 내가 너라도 그랬을 거야 \n",
      "=========> 살인자들과의 인터뷰\n",
      "=========> 잘못한 게 아니야, 잘 몰랐던 거야\n",
      "=========> 직업으로서의 소설가\n",
      "=========> [예스리커버] 정확한 사랑의 실험\n",
      "=========> 디즈니 프린세스, 내일의 너는 더 빛날 거야\n",
      "=========> 순례자 (스페셜 에디션)\n",
      "=========> 나, 열심히 살고 있는데 왜 자꾸 눈물이 나는 거니?\n",
      "=========> 편지할게요\n",
      "=========> 끌림\n",
      "=========> 온전히 나답게\n",
      "=========> 빨강머리 앤이 사랑한 풍경\n",
      "=========> 내가 멸종 위기인 줄도 모르고\n",
      "=========> 언제나 여행 중\n",
      "=========> 나를 잃지 마, 어떤 순간에도\n",
      "=========> 관계의 물리학\n",
      "=========> 한 글자 사전\n",
      "=========> 사려 깊은 수다\n",
      "=========> 나로 살아가는 기쁨\n",
      "=========> 뜨겁게 나를 응원한다\n",
      "# Page 14\n",
      "=========> 모든 요일의 기록 (10만 부 기념 리커버 에디션)\n",
      "=========> 당신을 응원하는 누군가\n",
      "=========> 내 나이가 어때서?\n",
      "=========> 이천 일의 휴가\n",
      "=========> 그냥 흘러넘쳐도 좋아요\n",
      "=========> 루시, 그래 인생의 주인공은 나야\n",
      "=========> 인생은 마카롱처럼\n",
      "=========> 엄마가 모르는 교사의 속마음\n",
      "=========> 라면을 끓이며\n",
      "=========> 연애하지 않을 권리\n",
      "=========> 나는 울 때마다 엄마 얼굴이 된다\n",
      "=========> 도시에서 죽는다는 것\n",
      "=========> 전쟁은 여자의 얼굴을 하지 않았다\n",
      "=========> 내 마음이 지옥일 때\n",
      "=========> 밥 한번 먹자 말하지만 얼굴 좀 보고 살잔 뜻입니다\n",
      "=========> 밤의 공항에서\n",
      "=========> 싫다면서 하고 있어 하하하\n",
      "=========> 빵 고르듯 살고 싶다\n",
      "=========> 다음 생엔 엄마의 엄마로 태어날게\n",
      "=========> 느리게 천천히 가도 괜찮아\n",
      "# Page 15\n",
      "=========> 너무 마음 바깥에 있었습니다\n",
      "=========> 바람이 불면 바람이 부는 나무가 되지요\n",
      "=========> 그날들이 참 좋았습니다\n",
      "=========> 건투를 빈다 (10주년 특별판)\n",
      "=========> 여행의 기술\n",
      "=========> 뷰티풀 보이\n",
      "=========> 자기만의 방\n",
      "=========> 앨리스, 너만의 길을 그려봐\n",
      "=========> 너의 숲이 되어줄게\n",
      "=========> 동전 하나로도 행복했던 구멍가게의 날들\n",
      "=========> 좋은 이별\n",
      "=========> 나의 영국 인문 기행\n",
      "=========> 피너츠 시리즈 완간 세트\n",
      "=========> 늘 다정한 사람 정작 내 마음은 돌보지 못하는 미련한 나에게\n",
      "=========> 보통은 이렇게 살고 있습니다 \n",
      "=========> 당신의 별자리는 무엇인가요\n",
      "=========> 모든 것이 마법처럼 괜찮아질 거라고\n",
      "=========> 뉴욕규림일기\n",
      "=========> 그럴 때 있으시죠?\n",
      "=========> 가문비나무의 노래 (리커버 한정판)\n",
      "# Page 16\n",
      "=========> 지독한 하루\n",
      "=========> 남자아이가 아니라  아이를 키우고 있습니다\n",
      "=========> 김진명의 한국사 X파일\n",
      "=========> 눈 감으면 보이는 것들 \n",
      "=========> 우드스탁, 작지만 이만하면 충분해 \n",
      "=========> 누구의 삶도 틀리지 않았다\n",
      "=========> 편안하고 사랑스럽고 그래 LOVE TMI BOOK\n",
      "=========> 당신은 이미 충분합니다\n",
      "=========> 빨강머리 앤이 하는 말 다이어리 북\n",
      "=========> 나는 왜 너를 사랑하는가\n",
      "=========> 자전거여행 1\n",
      "=========> 나는 왜 쓰는가\n",
      "=========> 야생 속으로\n",
      "=========> 밥장님! 어떻게 통영까지 가셨어요?\n",
      "=========> 읽다\n",
      "=========> 조화로운 삶\n",
      "=========> 돌아보니 삶은 아름다웠더라\n",
      "=========> 하루의 취향\n",
      "=========> 어쩌면 내가 가장 듣고 싶었던 말\n",
      "=========> 보다\n",
      "# Page 17\n",
      "=========> 이제 당신의 손을 보여줘요\n",
      "=========> 내 인생이 흔들린다 느껴진다면\n",
      "=========> 이렇게 살아도 돼\n",
      "=========> 선녀는 참지 않았다\n",
      "=========> 더 라스트 걸 THE LAST GIRL\n",
      "=========> 뜻밖의 위로\n",
      "=========> 사람의 목소리는 빛보다 멀리 간다\n",
      "=========> 어른도 기댈 곳이 필요해\n",
      "=========> 서른의 휴직\n",
      "=========> 내가 정말 알아야 할 모든 것은 유치원에서 배웠다\n",
      "=========> 졸업선물\n",
      "=========> 우리 가족은 꽤나 진지합니다\n",
      "=========> 훗날 내 청춘을 떠올리면, 네가 가장 먼저 생각날 거야\n",
      "=========> 타샤의 정원\n",
      "=========> 지도 밖으로 행군하라\n",
      "=========> 밤의 사색\n",
      "=========> 신영철 박사의 그냥 살자\n",
      "=========> 나는 천천히 아빠가 되었다\n",
      "=========> 딸바보가 그렸어, 엄마의 일기장\n",
      "=========> 우리들의 파리가 생각나요\n",
      "# Page 18\n",
      "=========> 나의 아름다운 비행\n",
      "=========> 물감을 사야 해서, 퇴사는 잠시 미뤘습니다\n",
      "=========> 그럴 땐 바로 토끼시죠\n",
      "=========> 다시 사랑하기 위한 말들\n",
      "=========> 아무튼, 외국어\n",
      "=========> 무탄트 메시지\n",
      "=========> 내 옆에 있는 사람\n",
      "=========> 소설가의 일\n",
      "=========> 유쾌한 수의사의 동물병원 24시\n",
      "=========> 문학하는 마음\n",
      "=========> 라이너스, 행복하기에도 모자란 하루야\n",
      "=========> 그렇게 우리는 간호사가 되어간다\n",
      "=========> 프로방스에서의 25년\n",
      "=========> 언젠가, 아마도\n",
      "=========> 보노보노의 인생상담 \n",
      "=========> 우린 누군가의 봄이었으니까\n",
      "=========> BTS: THE REVIEW\n",
      "=========> 조그맣게 살 거야\n",
      "=========> 나영석 피디의 어차피 레이스는 길다\n",
      "=========> 내가 얼마나 만만해 보였으면\n",
      "# Page 19\n",
      "=========> 네가 어떤 삶을 살든 나는 너를 응원할 것이다\n",
      "=========> 너에게 하고 싶은 말\n",
      "=========> 사는 게 뭐라고\n",
      "=========> 먼 북소리\n",
      "=========> 여자짐승아시아하기\n",
      "=========> 나는 천국을 보았다\n",
      "=========> 노땡큐\n",
      "=========> 달의 조각\n",
      "=========> 요가 매트만큼의 세계\n",
      "=========> 사람풍경\n",
      "=========> 법정 마음의 온도\n",
      "=========> 먹고 기도하고 사랑하라\n",
      "=========> 무너지지만 말아, 새벽 세시, 새삼스러운 세상 한정판 스페셜 에디션 세트\n",
      "=========> 한국대표수필 75\n",
      "=========> 시골의사의 아름다운 동행 1\n",
      "=========> 리얼 간호사 월드\n",
      "=========> 아무튼, 식물\n",
      "=========> 아무튼, 계속\n",
      "=========> 타샤 튜더, 나의 정원\n",
      "=========> 걷기예찬\n",
      "# Page 20\n",
      "=========> 내 우주는 온통 너였어\n",
      "=========> 엄마, 오늘도 사랑해\n",
      "=========> 섬\n",
      "=========> 아름다운 삶, 사랑 그리고 마무리\n",
      "=========> 사라짐, 맺힘\n",
      "=========> 기다리는 행복\n",
      "=========> 집에 왔습니다\n",
      "=========> 나보란 듯 사는 삶\n",
      "=========> 외로운 사람끼리 배추적을 먹었다\n",
      "=========> 좋다고 하니까 나도 좋다\n",
      "=========> 참 좋았다, 그치\n",
      "=========> 거의 정반대의 행복\n",
      "=========> 나와 너\n",
      "=========> 오늘도 계속 삽니다\n",
      "=========> 아흔일곱 번의 봄 여름 가을 겨울\n",
      "=========> 세 살배기 남편 그래도 사랑해\n",
      "=========> [예스리커버] 소박한 정원\n",
      "=========> 대한민국 소방관으로 산다는 것\n",
      "=========> 처음처럼\n",
      "=========> 금요일엔 돌아오렴\n",
      "# Page 21\n",
      "=========> 산티아고, 걷고 맛보고 요리하라\n",
      "=========> 내 얼굴을 만져도 괜찮은 너에게\n",
      "=========> 박가가 오늘도 수영일기\n",
      "=========> 나를 뺀 세상의 전부\n",
      "=========> 우리 함께하는 지금이 봄날\n",
      "=========> 아이돌의 작업실\n",
      "=========> 너 하고 싶은 거 다 해\n",
      "=========> 내가 소홀했던 것들\n",
      "=========> 경계에 흐르다\n",
      "=========> 살면서 쉬웠던 날은 단 하루도 없었다\n",
      "=========> 180도\n",
      "=========> 그릇을 비우고 나면 많은 것이 그리워졌다\n",
      "=========> 7년의 기록, 남자 간호사 데이비드 이야기\n",
      "=========> 아빠는 몰라두 돼\n",
      "=========> 너와 나의 1cm\n",
      "=========> 딸에게 보내는 굿나잇 키스\n",
      "=========> 고민 중독자들에게\n",
      "=========> 판결의 재구성\n",
      "=========> 모든 동물은 섹스 후 우울해진다\n",
      "=========> 아무도 미워하지 않는 개의 죽음\n",
      "# Page 22\n",
      "=========> #너에게\n",
      "=========> 어른인 척\n",
      "=========> 상실 수업\n",
      "=========> 안네의 일기\n",
      "=========> 페퍼민트 패티, 역시 인생은 쉽지 않구나\n",
      "=========> 우리를 행복하게 하는 것들\n",
      "=========> 도시를 걷는 문장들\n",
      "=========> 어떤 하루\n",
      "=========> 신과 나눈 이야기 3\n",
      "=========> 나를 망치는 나쁜 성실함\n",
      "=========> 진짜 그런 책은 없는데요\n",
      "=========> 삶이 내게 말을 걸어올 때\n",
      "=========> 오늘은 누구도 행복하지 않았으면 좋겠단 생각을 했습니다\n",
      "=========> 때때로 괜찮지 않았지만, 그래도 괜찮았어\n",
      "=========> 그런 책은 없는데요\n",
      "=========> 자유로울 것\n",
      "=========> 영혼을 위한 닭고기 수프 1 \n",
      "=========> 세상의 바보들에게 웃으면서 화내는 방법\n",
      "=========> 모든 일에는 이유가 있어\n",
      "=========> 기이한 것과 으스스한 것\n",
      "# Page 23\n",
      "=========> 마음의 구석\n",
      "=========> 걸어서 환장 속으로\n",
      "=========> 조금 우울하지만, 보통 사람입니다\n",
      "=========> 엄마 되기의 민낯\n",
      "=========> 아빠가 되었습니다만,\n",
      "=========> 처음 살아보니까 그럴 수 있어 (꽃길 스페셜 에디션)\n",
      "=========> 천 개의 공감\n",
      "=========> 하정우, 느낌 있다\n",
      "=========> 호젓한 시간의 만에서\n",
      "=========> 사소하지만 내 감정입니다\n",
      "=========> 닿음 Touch\n",
      "=========> 아기를 낳은 후에 남편을 미워하지 않는 법\n",
      "=========> 계절은 너에게 배웠어\n",
      "=========> 익숙해질 때\n",
      "=========> 진작 할 걸 그랬어\n",
      "=========> 마음에 따르지 말고 마음의 주인이 되어라\n",
      "=========> 우리집 테라스에 펭귄이 산다\n",
      "=========> 백석 평전\n",
      "=========> 홀가분\n",
      "=========> 내 인생에 힘이 되어준 한마디\n",
      "# Page 24\n",
      "=========> 에이틴 2 포토에세이\n",
      "=========> 당신이 문득 길고양이와 마주친다면\n",
      "=========> 축하해\n",
      "=========> 아임 파인, 앤유?\n",
      "=========> 아직, 도쿄\n",
      "=========> 너를 만나 삶이 맛나\n",
      "=========> Evasions  행복을 찾아서\n",
      "=========> 엄마에게 안부를 묻는 밤\n",
      "=========> 네이비씰의 나를 이기는 연습\n",
      "=========> 당신, 전생에서 읽어드립니다\n",
      "=========> 정원에서 보내는 시간\n",
      "=========> 지금은 없는 이야기\n",
      "=========> 나는 지금 누구를 사랑하는가\n",
      "=========> 꿈 너머 꿈\n",
      "=========> 내 생애 단 한번\n",
      "=========> 문학의 숲을 거닐다\n",
      "=========> 묵묵\n",
      "=========> 서민의 개좋음\n",
      "=========> 전국 책방 여행기\n",
      "=========> 신과 나눈 이야기 2\n",
      "# Page 25\n",
      "=========> 두 명은 아니지만 둘이 살아요\n",
      "=========> 나는 내가 잘됐으면 좋겠다\n",
      "=========> 좋은 아침 같은 소리 하고 있네\n",
      "=========> 내가 말해 줄게요\n",
      "=========> 수영일기\n",
      "=========> 야단법석 2\n",
      "=========> 안 느끼한 산문집\n",
      "=========> 나는 걷기로 했다\n",
      "=========> 별빛에 꿈을 담고\n",
      "=========> 간호사, 너 자신이 되어라 \n",
      "=========> 철학자와 늑대\n",
      "=========> 당신의 슬픔을 훔칠게요\n",
      "=========> 그 순간 그 문장이 떠올랐다\n",
      "=========> 문지방을 넘어서\n",
      "=========> 평양, 제가 한번 가보겠습니다\n",
      "=========> 지금도 어린, 어린왕자\n",
      "=========> 안아주는 정원\n",
      "=========> 타샤의 말\n",
      "=========> 우리는 사랑 아니면 여행이겠지\n",
      "=========> 넌 괜찮겠지만 난 아니라고\n",
      "# Page 26\n",
      "=========> 애도 일기\n",
      "=========> 실은 괜찮지 않았던 날들\n",
      "=========> 혼자서 본 영화\n",
      "=========> 블랙코미디\n",
      "=========> 힐빌리의 노래\n",
      "=========> 완벽하지 않은 것들에 대한 사랑 큰글씨책\n",
      "=========> 당신이 빛이라면\n",
      "=========> 일단은 즐기고 보련다\n",
      "=========> 내가 원하는 삶을 살았더라면\n",
      "=========> 눈물도 빛을 만나면 반짝인다\n",
      "=========> 파일럿의 특별한 비행일지\n",
      "=========> 하면 좋습니까?\n",
      "=========> 아직 오지 않은 소설가에게\n",
      "=========> 유연하게 흔들리는 중입니다\n",
      "=========> 남미 히피 로드\n",
      "=========> 그 찬란한 빛들 모두 사라진다 해도\n",
      "=========> 또 이 따위 레시피라니\n",
      "=========> 지금부터 재판을 시작하겠습니다\n",
      "=========> 오늘 뭐 먹지?\n",
      "=========> 지지 않는다는 말\n",
      "# Page 27\n",
      "=========> 아무튼, 서재\n",
      "=========> 인문학으로 광고하다\n",
      "=========> 너에게 사랑을 배운다\n",
      "=========> 이런 말이 얼마나 위로가 될지는 모르겠지만\n",
      "=========> 키크니의 무엇이든 그려드립니닷!\n",
      "=========> 무탈한 오늘\n",
      "=========> 잘돼가? 무엇이든\n",
      "=========> 어디까지나 제 생각입니다\n",
      "=========> 간다, 봐라\n",
      "=========> 김수영 전집 2\n",
      "=========> 인생에서 너무 늦은 때란 없습니다 엽서책\n",
      "=========> 나를 부르는 숲\n",
      "=========> 서서히 서서히 그러나 반드시\n",
      "=========> 스물셋, 죽기로 결심하다\n",
      "=========> 진실이 치유한다\n",
      "=========> 사랑할 때 알아야 할 것들\n",
      "=========> 딸 바보가 그렸어\n",
      "=========> 한글자\n",
      "=========> 지선아 사랑해\n",
      "=========> 우리가 사랑한 세상의 모든 책들\n",
      "# Page 28\n",
      "=========> 별걸 다 기억하는\n",
      "=========> 친애하는 미스터 최\n",
      "=========> 스물 셋, 지금부터 혼자 삽니다\n",
      "=========> 오늘의 메뉴는 제철 음식입니다\n",
      "=========> 사람을 미워하는 가장 다정한 방식\n",
      "=========> 안녕하세요 내 이름은 인절미예요\n",
      "=========> 혼자 있기 좋은 방\n",
      "=========> 비밀편지\n",
      "=========> 자전거여행 2\n",
      "=========> 시옷의 세계\n",
      "=========> 여행하지 않은 곳에 대해 말하는 법\n",
      "=========> 아기 말고 내 몸이 궁금해서\n",
      "=========> 그 여름, 7일\n",
      "=========> 뉴욕에서 간호사로 살아보기\n",
      "=========> 나는 매일 뉴욕 간다\n",
      "=========> 찔레꽃 그여자\n",
      "=========> 영원한 외출\n",
      "=========> 바이올린과 순례자\n",
      "=========> 이 순간이 끝나지 않았으면 좋겠어\n",
      "=========> 나는 당신이 살았으면 좋겠습니다\n",
      "# Page 29\n",
      "=========> 너에게 들키고 싶은 혼잣말\n",
      "=========> 편안하고 사랑스럽고 그래 세트\n",
      "=========> 나 안 괜찮아\n",
      "=========> 떠난 후에 남겨진 것들\n",
      "=========> 내 인생에 용기가 되어준 한마디\n",
      "=========> 안네의 일기\n",
      "=========> 인턴일기\n",
      "=========> 사람아 아, 사람아!\n",
      "=========> 권태를 모르는 위대한 노동자\n",
      "=========> 나는 엄마가 먹여 살렸는데\n",
      "=========> 이지 클래식\n",
      "=========> 비가 와도 꽃은 피듯이\n",
      "=========> 나는 여경이 아니라 경찰관입니다\n",
      "=========> 오늘 내가 사는 게 재미있는 이유\n",
      "=========> 글로벌 거지 부부\n",
      "=========> 읽지 않은 책에 대해 말하는 법\n",
      "=========> 산의 마음을 배우다\n",
      "=========> 신입사원 빵떡씨의 극비 일기\n",
      "=========> 스폰지밥, 언제나 내 마음대로 즐거워\n",
      "=========> 오늘 아내에게 우울증이라고 말했다\n",
      "# Page 30\n",
      "=========> Flying High\n",
      "=========> 미키 마우스, 오늘부터 멋진 인생이 시작될 거야\n",
      "=========> 우리가 함께 걷는 시간\n",
      "=========> 말과 마음 사이\n",
      "=========> 오케이 라이프\n",
      "=========> 방황해도 괜찮아\n",
      "=========> 달의 위로 + 마음이 마음에게 + 미안해 사랑해 고마워 : 리미티드 에디션 세트\n",
      "=========> 별일 아닌 것들로 별일이 됐던 어느 밤\n",
      "=========> 엄마라고 불러줘서 고마워 \n",
      "=========> 무너지지만 말아\n",
      "=========> 너는 물처럼 내게 밀려오라\n",
      "=========> 이렇게 작지만 확실한 행복\n",
      "=========> 밤 열한 시\n",
      "=========> 마법의 순간\n",
      "=========> 1리터의 눈물\n",
      "=========> 김대중 자서전 1\n",
      "=========> 스페인 기행\n",
      "=========> 주기율표\n",
      "=========> 아무튼, 발레\n",
      "=========> 좋은 사람이길 포기하면 편안해지지\n",
      "# Page 31\n",
      "=========> 아이 캔 주짓수\n",
      "=========> 댕댕이 친구들! 이탈리아 여행가개!\n",
      "=========> 저 이래 봬도 잘 살고 있습니다\n",
      "=========> 퇴사는 여행\n",
      "=========> 나는 당신들의 아랫사람이 아닙니다\n",
      "=========> 이웃집의 백호\n",
      "=========> 집에만 있긴 싫고\n",
      "=========> 희망 대신 욕망\n",
      "=========> 함부로 대하는 사람들에게 조용히 갚아주는 법\n",
      "=========> 당선, 합격, 계급\n",
      "=========> 스페인 너는 자유다\n",
      "=========> 나는 뭘 기대한 걸까\n",
      "=========> 스폰지밥, 너와 함께라서 더 좋아\n",
      "=========> 키르케고르, 나로 존재하는 용기\n",
      "=========> 왕이 된 남자 포토에세이\n",
      "=========> 오늘도 출근을 해냅니다\n",
      "=========> 매일 갑니다, 편의점\n",
      "=========> 그대는 할말을 어디에 두고 왔는가\n",
      "=========> 감정에 체한 밤\n",
      "=========> 틈만 나면 딴생각\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-52db475cd688>:119: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  df = pd.DataFrame(np.array(book_list), columns=colList)\n"
     ]
    }
   ],
   "source": [
    "#가정 살림 : 001001001\n",
    "#건강 취미 : 001001011\n",
    "#경제 경영 : 001001025\n",
    "#국어 외국어 사전 : 001001004\n",
    "#대학교재 : 001001014\n",
    "#사회 정치 : 001001022\n",
    "#소설/시/희곡 : 001001046\n",
    "#수험서 자격증 : 001001015\n",
    "#어린이 : 001001016\n",
    "#에세이 : 001001047\n",
    "#여행 : 001001009\n",
    "#역사 : 001001010\n",
    "#예술 : 001001007\n",
    "#유아 : 001001027\n",
    "#인문 : 001001019\n",
    "#인물 : 001001020\n",
    "#자기계발 : 001001026\n",
    "#자연과학 : 001001002\n",
    "#잡지 : 001001024\n",
    "#종교 : 001001021\n",
    "#청소년 : 001001005\n",
    "#IT 모바일 : 001001003\n",
    "#초등참고서 : 001001044\n",
    "#중고등참고서 : 001001013\n",
    "\n",
    "\n",
    "CategoryNum= '001001047'\n",
    "\n",
    "# 2019년도\n",
    "for year in range(2019, 2020):\n",
    "    print('='*50)\n",
    "    print('# Year', year)\n",
    "    print('='*50)\n",
    "\n",
    "    # 9월\n",
    "    for month in range(9, 10):\n",
    "        print('='*50)\n",
    "        print('# Month', month)\n",
    "        print('='*50)\n",
    "\n",
    "        # 월 별 데이터 수집\n",
    "        df = yes24DataReader(CategoryNum, str(year), str(month))\n",
    "        \n",
    "        #db연결\n",
    "        #con = sqlite3.connect(\"/Users/parkjiyun/PycharmProjects/pythonProject/db.sqlite3\")\n",
    "        #df.to_sql(str(year)+'_'+str(month)+'_'+str(CategoryNum), con, if_exists='replace')\n",
    "        # 월 별로 수집된 데이터를 CSV 형식 파일로 저장\n",
    "        df.to_csv(str(year)+'_'+str(month)+'_'+str(CategoryNum)+'.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6282ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜 새삼 \"역사\"인가?  대한민국의 근현대사를 파헤치다!지금의 대한민국은 어떤가? 한국의 정치와 사회는 깊은 뿌리서부터 흔들리고 분열을 안고 있으며, 그것은 끊임없이 정치와 사회의 크고 작은 갈등과 대립으로 드러나고 있다. 우리가 제대로 된 나라의 기초이념을, 그것이 어떻게 생겨났으며, 그 이념을 어떤 정치세력이 받들어 국가를 세웠는지를 정립했다면 지금처럼 분열되고 흔들릴까. 저자는 왜 새삼 \"대한민국의 역사\"가 필요한 건지 인식하고 있으며, 또한  지금까지 쓰이고 가르쳐진 대한민국의 역사는 이 나라가 세워지고 발전해온 역사를 정당하게 평가하지 않았다고 말한다. 국민을 흩뜨리는 분열의 역사가 아닌 통합의 역사를 새롭게 쓸 필요가 있다고 생각한 것이다.  대한민국정부수립 이후 한국의 기록을 그려낸 이 책은 경제성장과 민주주의, 5·16군사정변, 근대화, 유신체제까지 대한민국이 걸어온 길과 발자취를 담았다.\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from konlpy.tag import Okt\n",
    "# from konlpy.tag import Kkma\n",
    "\n",
    "# # url = 'http://www.yes24.com/Product/Goods/44499829'\n",
    "# # res = requests.post(url)\n",
    "# # soup = BeautifulSoup(res.text, 'html5lib')\n",
    "# # tag_description = '#infoset_introduce > div.infoSetCont_wrap > div.infoWrap_txt'\n",
    "\n",
    "# # books = soup.select(tag_description)\n",
    "\n",
    "# #description = books[0].get_text()\n",
    "# description = '<b>왜 새삼 \"역사\"인가?  대한민국의 근현대사를 파헤치다!</b><br/><br/>지금의 대한민국은 어떤가? 한국의 정치와 사회는 깊은 뿌리서부터 흔들리고 분열을 안고 있으며, 그것은 끊임없이 정치와 사회의 크고 작은 갈등과 대립으로 드러나고 있다. 우리가 제대로 된 나라의 기초이념을, 그것이 어떻게 생겨났으며, 그 이념을 어떤 정치세력이 받들어 국가를 세웠는지를 정립했다면 지금처럼 분열되고 흔들릴까. 저자는 왜 새삼 \"대한민국의 역사\"가 필요한 건지 인식하고 있으며, 또한  지금까지 쓰이고 가르쳐진 대한민국의 역사는 이 나라가 세워지고 발전해온 역사를 정당하게 평가하지 않았다고 말한다. 국민을 흩뜨리는 분열의 역사가 아닌 통합의 역사를 새롭게 쓸 필요가 있다고 생각한 것이다.  대한민국정부수립 이후 한국의 기록을 그려낸 이 책은 경제성장과 민주주의, 5·16군사정변, 근대화, 유신체제까지 대한민국이 걸어온 길과 발자취를 담았다.'\n",
    "# characters = \"<b></br>/</b><br>\"\n",
    "# description = ''.join( x for x in description if x not in characters)\n",
    "# #description = description.replace('<b>', '').replace('<br>', '').replace('</br>', '').replace('/', '').replace('</b>', '')\n",
    "# print(description)\n",
    "\n",
    "# # from konlpy.tag import Okt\n",
    "# # okt_pos = Okt().pos(description, norm=True, stem=True)   # 형태소 분석 ( norm : 정규화 )\n",
    "# # okt_filtering = [x for x, y in okt_pos if y in ['Noun', 'Adjective'] if len(x) >= 2 if x not in ['있다', '그것']]\n",
    "# # tokenizer = \" \".join(okt_filtering)\n",
    "# # print(tokenizer)\n",
    "\n",
    "# # description = '<B>‘한국의 산사’, 유네스코 세계유산 등재 기념 특별판!<br/>일찍이 산사를 예찬해온 유홍준의 ‘답사기’, 그 절정만 가려뽑은 단 한 권!<br/></B><br/>1994년 제1권 ‘남도답사 일번지’를 발간한 이래 올해까지 누적 판매부수 400만부를 넘긴 국내 최장수 베스트셀러 시리즈 『나의 문화유산답사기』는 국내편 10권에 걸쳐 우리나라 각지의 ‘산사’를 가장 열심히 공들여 소개한 바 있다. 저자 유홍준 교수는 우리 산사의 가치와 아름다움에 주목하고, 널리 알리고, 예찬해 마지않던 국내 최고의 전문가이기도 하다. 지난 6월 우리 산사 7곳이 유네스코 세계유산에 등재되었으니, 이제 ‘산사’는 우리만의 문화유산이 아니라 세계가 인정하는 자랑스러운 문화유산이 되었다. <br/><br/>유네스코 세계유산 등재를 기념하여 그간 ‘답사기’에 실렸던 남한의 대표적인 산사 20여 곳과, 아직은 가볼 수 없지만 언젠가는 가보게 될 북한의 산사 2곳을 가려뽑았다. 오랜 시간에 걸쳐 집필한 글들을 모아, 오늘의 독자들이 우리 산사의 가치와 역사 등에 대한 이야기에 집중하도록 새롭게 선보인 것이다. 전국 어느 산을 가든 으레 산사를 만나는 우리나라, 산사의 아름다움을 직접 느낄 수 있는 올가을 답삿길에 충실하고 살뜰한 길잡이가 되리라 기대한다.'\n",
    "\n",
    "# # print(description)\n",
    "# # stopwords = '아 어제 가장 가인 마주 역사 비롯 것임 많다 일인 위함 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓'\n",
    "                \n",
    "# # # # 형태소 분석 ( norm : 정규화 ) /형태소 단위로 쪼개고 각 품사를 태깅해서 리스트형태로 반한   \n",
    "# # okt_pos = Okt().pos(description, norm=True, stem=True)\n",
    "# # #print(okt_pos)\n",
    "# # #품사가 명사와 형용사일 때, 단어길이가 2이상일 때 추출\n",
    "# # okt_filtering = [x for x, y in okt_pos if y in ['Noun','Adjective'] if len(x) >= 2 if x not in stopwords]\n",
    "# # print(okt_filtering)\n",
    "            \n",
    "# # # tokenizer = \" \".join(okt_filtering)\n",
    "# # # print(tokenizer)\n",
    "\n",
    "# # # count_dict = [(token, description.count(token)) for token in okt_filtering ]\n",
    "# # kkma_pos = Kkma().pos(description)\n",
    "# # #print(kkma_pos)\n",
    "# # kkma_filtering = [x for x, y in kkma_pos if y in ['NNG','NNP','NNB', 'VA'] if len(x) >= 2 if x not in stopwords]\n",
    "# # print(kkma_filtering)\n",
    "# # # tokenizer = \" \".join(kkma_filtering)\n",
    "# # # print(tokenizer)\n",
    "# # # count_dict = [(token, description.count(token)) for token in kkma_filtering ]\n",
    "            \n",
    "            \n",
    "\n",
    "# # # print(count_dict)\n",
    "# # # ranked_words = sorted(count_dict, key=lambda x:x[1], reverse=True)[:30]\n",
    "# # # ranked_words = set(ranked_words)\n",
    "# # # print(ranked_words)\n",
    "# # # keyword = [ keyword for keyword, freq in ranked_words ]\n",
    "# # # print(keyword)\n",
    "# # # print(keyword[:5])\n",
    "# # # print(sorted(keyword)[:5])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d912d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<', 'Punctuation')\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Okt\n",
    "# #description = '<B>2008년 금융위기를 이해하지 못하면 오늘의 세계를 이해할 수 없다!<br/>우크라이나 위기, 브렉시트 국민투표, 트럼프의 당선까지<br/>경제사가의 눈으로 그려낸 최근 10년의 세계사이자 경제 대서사시<br/></B><br/>『붕괴(Crashed)』는 현대 자본주의 체제와 글로벌 경제의 한복판에서 벌어진 2008년 금융위기 이후 10년 역사를 다룬다. 이 책의 저자인 컬럼비아대학교 애덤 투즈(Adam Tooze) 교수는 경제사 분야에서 세계적으로 주목받는 학자로서 위기의 진앙인 미국과 유럽은 물론 중국과 러시아, 신흥시장국가에 이르기까지 전 지구적 규모로 확산하는 금융위기의 진행 상황을 치밀하게 그려내는 한편, 위기 대응의 과정과 방법도 꼼꼼하게 진단함으로써 세계의 경제와 정치가 긴밀히 얽힌 오늘의 세계를 분명히 알려준다. 또한 『붕괴』는 조지아와 우크라이나 위기, 브렉시트 국민투표, 트럼프의 당선에 이르기까지 최근 10년의 세계사적 사건들을 금융위기와의 관련 속에서 유려하게 풀어쓴 “경제 대서사시”다. 경제학자 존 케네스 갤브레이스는 “금융의 세계만큼 역사의 교훈으로부터 가르침을 받지 못한 분야도 없다”고 지적한 바 있는데, 투즈 교수는 경제사가의 통찰과 안목으로 변혁의 시기를 헤쳐 갈 지혜를 책에 담아냈다. <br/><br/>애덤 투즈 교수는 내놓은 책마다 숱한 화제를 뿌리며 세간의 주목과 수상의 영예를 동시에 누려왔다. 그중에서도 『대재앙(Deludge)』(2014)은 투즈에게 “위대한 역사가의 탄생”이라는 칭호를 안긴 저작으로서 제1차 세계대전 후 미국을 중심으로 재편되는 국제질서(팍스 아메리카나)를 인상적으로 그려냈다. 이러한 전간기(interwar)의 진단은 칼 폴라니, E.H. 카, 피터 테민과 같은 20세기의 지성들이 했던 작업이다.<br/><br/>『붕괴』는 출간 직후 『가디언』, 『파이낸셜타임스』, 『뉴욕타임스』, 『월스트리트저널』 등 영미권의 주요 언론에서 화제작으로 소개되었고 ‘『뉴욕타임스』 올해의 주목할 만한 책’ ‘『이코노미스트』 올해의 책’에 선정되었다. 투즈는 전간기 연구의 전문가라는 명성에 더하여 이 『붕괴』의 저술로 최고 권위의 국제문제 분야의 논픽션에 주어지는 라이오넬겔버상을 수상함으로써 역대 수상자인 조나단 스펜서, 에릭 홉스봄, 니얼 퍼거슨 등과 어깨를 나란히 했으며 외교전문지 『포린폴리시』가 선정한 ‘세계의 사상가(Global Thinker) 100인’에 뽑혔다.'\n",
    "# description = '<B>유신 전야의 쓰나미 앞에 일본 막부가 내린 결단은?</B><br/><br/>막부는 존왕양이파의 대표 세력인 조슈 번에 본때를 보이겠다며 35개 번 15만 대군을 일으킨다. 일본 앞바다 효고를 개항하라는 영국의 압박에 천황 고메이는 ‘절대 양이’를 강력히 외치고, 막부의 로주를 파면·개역하라는 당황스러운 명을 내린다. 이에 화들짝 놀란 쇼군 이에모치는 조정에 사표를 내미는데…. 이 어지러움에 막부의 머리가 터질 지경이다. 일본은 이 거대한 소용돌이에서 무사히 빠져나올 수 있을까?'\n",
    "# okt_pos = Okt().pos(description, norm=True, stem=True)\n",
    "# for i in len(okt_pos):\n",
    "#     if okt_pos[i] == ''\n",
    "# print(okt_pos[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b79cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('무라카미', 1), ('무라카미 하루키', 1), ('무라카미 하루키의 대표작', 1), ('노르웨이', 2), ('노르웨이의 숲', 2), ('1987년', 1), ('1987년 발표', 1), ('1987년 발표된 후', 1), ('하루키', 2), ('하루키 붐', 1), ('저자', 1), ('저자의 문학적 성과', 1), ('널리', 1), ('널리 알린 현대', 1), ('널리 알린 현대 일본', 1), ('널리 알린 현대 일본 문학', 1), ('널리 알린 현대 일본 문학의 대표작', 1), ('원문', 1), ('언어', 1), ('번역', 1), ('첫 만남', 1), ('추억', 1), ('추억하는 독자', 1), ('추억하는 독자와 새로운 만남', 1), ('독자', 2), ('독자 모두', 1), ('못할 기억', 1), ('독일', 1), ('독일 함부르크', 1), ('독일 함부르크 공항', 1), ('막 착륙', 1), ('비행기', 1), ('비행기 안', 1), ('비틀스', 1), ('와타나베', 2), ('자신', 2), ('간절한 부탁', 1), ('여자', 2), ('여자와 그', 1), ('여자와 그 부탁', 1), ('고등학교', 1), ('고등학교 시절', 1), ('고등학교 시절 친한 친구', 1), ('고등학교 시절 친한 친구 기즈키', 1), ('그의 여자', 1), ('그의 여자 친구', 1), ('기즈키', 2), ('기즈키의 갑작스러운 자살', 1), ('행복한 시간', 1), ('도쿄', 1), ('도쿄의 사립대학', 1), ('진학', 1), ('고향', 1), ('나타', 1), ('연락', 1), ('요양원', 1), ('코의 편지', 1), ('대표', 2), ('발표', 1), ('성과', 1), ('현대', 2), ('일본', 1), ('문학', 2), ('만남', 2), ('모두', 1), ('기억', 1), ('함부르크', 1), ('공항', 1), ('착륙', 1), ('부탁', 2), ('시절', 1), ('친구', 2), ('자살', 1), ('시간', 1), ('사립', 1), ('대학', 1), ('편지', 1)]\n",
      "['노르웨이', '노르웨이의 숲', '하루키', '독자', '와타나베']\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Okt\n",
    "# #text = '역사는 반복된다. 한국과 일본의 무역 문제, 중국의 반일 감정, 중동의 크고 작은 분쟁, 나치 독일의 문제 등 현재 뉴스를 장식하는 이슈를 하나하나 따라가다 보면 단지 오늘날의 문제가 아니라 역사에서 비롯된 것임을 깨닫게 된다. 역사를 아는 사람만이 현 시대를 이해할 수 있다. <br/><br/>이 책은 세계의 역사에서 가장 다이내믹한 시기인 1850~1960년, 가장 중요한 순간들 200장면을 담았다. 이 시기는 사진의 기록이 남게 된 최초의 시기이나, 안타깝게도 흑백사진으로만 기록이 남은 시대다. 저자들은 흑백으로만 기억되는 격동기 세계사를 컬러로 복원하여 ‘역사의 색’을 찾고자 했다. 역사의 색을 찾는다는 것은 단순히 색을 복원하는 차원의 것이 아니다. 컬러풀한 세계사를 보여주면서 빛바랜 흑백의 역사를 되살려 우리에게 그 의미와 가치를 확고하고도 분명하게 알려주기 위함이다. 사진 한 장이 보여주는 것은 생각보다 많다. 그리고 역사가 제 빛을 찾았을 때 그 가치는 더 선명해진다. 잔인한 전장의 모습과 희생자의 표정들, 수용소의 처참한 얼굴들, 혁명가가 만나던 찰나의 순간, 그 모든 최초의 기록들. 컬러로 마주한 역사의 순간은 강렬하고 흥미롭다. 그리하여 우리에 뇌리에 선명하게 새겨진다.<br/>이 책은 이미지를 기초로 역사를 기억하게 해주는 놀라운 역사책이다. 영국 베스트셀러 작가이자 역사가인 댄 존스의 간단명료한 해설은 어렵게만 느껴졌던 현대사를 평생 내 것이 되게 만든다. 연대 순으로, 키워드 순으로, 인물 순으로 역사를 기억하는 책은 있었지만, 선명한 사진을 중심으로 역사를 기억하는 역사책은 지금껏 없었다.<br/><br/>역사를 통해 현재를 알고 싶은 역알못이 있다면, 이 책을 권한다. 색을 되찾은 역사는 바로 어제의 일인 듯 생명력을 지닌 채 우리에게 다가온다. 색의 힘은 상상 그 이상이다.'\n",
    "# text = '무라카미 하루키의 대표작 『노르웨이의 숲』. 1987년 발표된 후 세계적인 ‘하루키 붐’을 일으키며 저자의 문학적 성과를 널리 알린 현대 일본 문학의 대표작이다. 원문에 충실하면서도 현대적인 언어로 새롭게 번역한 이 책은 첫 만남을 추억하는 독자와 새로운 만남을 기다리는 독자 모두에게 잊지 못할 기억을 전해준다. 독일 함부르크 공항에 막 착륙한 비행기 안에서 울린 비틀스의 《노르웨이의 숲》을 듣고 와타나베는 자신에게 간절한 부탁을 남긴 여자와 그 부탁을 떠올린다. 고등학교 시절 친한 친구 기즈키, 그의 여자 친구 나오코와 언제나 함께였던 와타나베. 그러나 기즈키의 갑작스러운 자살로 행복한 시간은 끝나 버리고 만다. 도쿄의 사립대학에 진학해 고향을 떠나온 와나타베는 나오코와 한동안 연락을 끊고 지내던 어느 날, 자신이 요양원에 들어가 있다는 나오코의 편지를 받게 되는데…….'\n",
    "\n",
    "# def keyword_extractor(tagger, text):\n",
    "#     tokens = tagger.phrases(text)\n",
    "#     tokens = [ token for token in tokens if len(token) > 1  ] # 한 글자인 단어는 제외\n",
    "#     count_dict = [(token, text.count(token)) for token in tokens ]\n",
    "#     print(count_dict)\n",
    "#     ranked_words = sorted(count_dict, key=lambda x:x[1], reverse=True)[:5]\n",
    "#     return [ keyword for keyword, freq in ranked_words ]\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     twit = Okt()\n",
    "#     print( keyword_extractor(twit, text) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84899b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "                            “세월이 이따금 묻는다 사랑은 그 후 어떻게 되었느냐고”[그대가 곁에 있어도 나는 그대가 그립다]에 이은류시화 시인의 두 번째 시집1997년에 출간한 류시화 시인의 두 번째 시집 개정판이다. 시인이 서문에 썼듯이 초판본에 실었던 시들 중에서 여러 편을 수정하고, '마음에 들지 않는 시'들을 덜어냈다. 그러나 시 속에 담긴 시인의 시선은 변함이 없다. 아무리 해도 '버려지지 않는 것들'이 있기 때문일 것이다. 시인은 쓸쓸한 언어로 화사한 인생을 노래해야 하는 운명을 지닌 사람이다. 그래서 시는 이상하다. '삶에는 시로써만 말할 수 있는 것'이 있으며, '사물들은 시인을 통해 말하고 싶어 한다'는 것이 류시화의 일관된 시정신이다. 여기, 특별한 시적 감각과 삶을 응시하는 독특한 시선, 그리고 이 세계를 마주한 사유가 돋보이는 48편의 시가 있다. 가수 안치환이 노래로 부른 [소금인형]처럼 류시화의 시는 소리 내어 읽을 때 더 울림이 크다.                    \n",
      "                        \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from konlpy.tag import Okt\n",
    "\n",
    "# url = 'http://www.yes24.com/Product/Goods/26932917'\n",
    "# res = requests.post(url)\n",
    "# soup = BeautifulSoup(res.text, 'html5lib')\n",
    "# tag_description = '#infoset_introduce > div.infoSetCont_wrap > div.infoWrap_txt'\n",
    "\n",
    "# books = soup.select(tag_description)\n",
    "\n",
    "# description = books[0].get_text()\n",
    "# characters = \"<b></br>/</b><br>\"\n",
    "# description = ''.join( x for x in description if x not in characters)\n",
    "# #description = description.replace('<b>', '').replace('<br>', '').replace('</br>', '').replace('/', '').replace('</b>', '')\n",
    "# print(description)\n",
    "\n",
    "\n",
    "# # def keyword_extractor(tagger, text):\n",
    "# #     tokens = tagger.phrases(text)\n",
    "# #     print(tokens)\n",
    "# #     tokens = [ token for token in tokens if len(token) > 1 ] # 한 글자인 단어는 제외\n",
    "# #     count_dict = [(token, text.count(token)) for token in tokens ]\n",
    "# #     print(count_dict)\n",
    "# #     ranked_words = sorted(count_dict, key=lambda x:x[1], reverse=True)[:5]\n",
    "# #     return [ keyword for keyword, freq in ranked_words ]\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "# #     twit = Okt()\n",
    "# #     print( keyword_extractor(twit, description))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6df6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['반복', '한국', '일본', '무역', '중국', '반일', '감정', '중동', '분쟁', '나치', '독일', '현재', '뉴스', '장식', '이슈', '오늘날', '시대', '이해', '세계', '다이내믹', '시기', '순간', '장면', '시기', '사진', '기록', '최초', '시기', '흑백사진', '기록', '시대', '저자', '흑백', '기억', '격동', '세계사', '컬러', '복원', '복원', '차원', '컬러풀', '세계사', '빛바랜', '흑백', '의미', '가치', '확고', '사진', '가치', '전장', '모습', '희생', '표정', '용소', '얼굴', '혁명가', '찰나', '순간', '모든', '최초', '기록', '컬러', '순간', '뇌리', '이미지', '기초', '기억', '영국', '베스트셀러', '작가', '이자', '존스', '간단명료', '해설', '대사', '평생', '연대', '워드', '인물', '기억', '사진', '중심', '기억', '지금껏', '통해', '현재', '생명력', '상상']\n",
      "반복 한국 일본 무역 중국 반일 감정 중동 분쟁 나치 독일 현재 뉴스 장식 이슈 오늘날 시대 이해 세계 다이내믹 시기 순간 장면 시기 사진 기록 최초 시기 흑백사진 기록 시대 저자 흑백 기억 격동 세계사 컬러 복원 복원 차원 컬러풀 세계사 빛바랜 흑백 의미 가치 확고 사진 가치 전장 모습 희생 표정 용소 얼굴 혁명가 찰나 순간 모든 최초 기록 컬러 순간 뇌리 이미지 기초 기억 영국 베스트셀러 작가 이자 존스 간단명료 해설 대사 평생 연대 워드 인물 기억 사진 중심 기억 지금껏 통해 현재 생명력 상상\n",
      "      Number\n",
      "기억  0.338062\n",
      "시기  0.253546\n",
      "기록  0.253546\n",
      "순간  0.253546\n",
      "사진  0.253546\n",
      "0.3380617018914066\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from konlpy.tag import Okt\n",
    "\n",
    "# description = '역사는 반복된다. 한국과 일본의 무역 문제, 중국의 반일 감정, 중동의 크고 작은 분쟁, 나치 독일의 문제 등 현재 뉴스를 장식하는 이슈를 하나하나 따라가다 보면 단지 오늘날의 문제가 아니라 역사에서 비롯된 것임을 깨닫게 된다. 역사를 아는 사람만이 현 시대를 이해할 수 있다. <br/><br/>이 책은 세계의 역사에서 가장 다이내믹한 시기인 1850~1960년, 가장 중요한 순간들 200장면을 담았다. 이 시기는 사진의 기록이 남게 된 최초의 시기이나, 안타깝게도 흑백사진으로만 기록이 남은 시대다. 저자들은 흑백으로만 기억되는 격동기 세계사를 컬러로 복원하여 \"역사의 색\"을 찾고자 했다. 역사의 색을 찾는다는 것은 단순히 색을 복원하는 차원의 것이 아니다. 컬러풀한 세계사를 보여주면서 빛바랜 흑백의 역사를 되살려 우리에게 그 의미와 가치를 확고하고도 분명하게 알려주기 위함이다. 사진 한 장이 보여주는 것은 생각보다 많다. 그리고 역사가 제 빛을 찾았을 때 그 가치는 더 선명해진다. 잔인한 전장의 모습과 희생자의 표정들, 수용소의 처참한 얼굴들, 혁명가가 만나던 찰나의 순간, 그 모든 최초의 기록들. 컬러로 마주한 역사의 순간은 강렬하고 흥미롭다. 그리하여 우리에 뇌리에 선명하게 새겨진다.<br/>이 책은 이미지를 기초로 역사를 기억하게 해주는 놀라운 역사책이다. 영국 베스트셀러 작가이자 역사가인 댄 존스의 간단명료한 해설은 어렵게만 느껴졌던 현대사를 평생 내 것이 되게 만든다. 연대 순으로, 키워드 순으로, 인물 순으로 역사를 기억하는 책은 있었지만, 선명한 사진을 중심으로 역사를 기억하는 역사책은 지금껏 없었다.<br/><br/>역사를 통해 현재를 알고 싶은 역알못이 있다면, 이 책을 권한다. 색을 되찾은 역사는 바로 어제의 일인 듯 생명력을 지닌 채 우리에게 다가온다. 색의 힘은 상상 그 이상이다.'\n",
    "# stopwords = '아 어제 가장 가인 마주 역사 비롯 것임 많다 일인 위함 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓'\n",
    "                \n",
    "# # 형태소 분석 ( norm : 정규화 ) /형태소 단위로 쪼개고 각 품사를 태깅해서 리스트형태로 반한   \n",
    "# okt_pos = Okt().pos(description, norm=True, stem=True)\n",
    "# #품사가 명사와 형용사일 때, 단어길이가 2이상일 때 추출\n",
    "# okt_filtering = [x for x, y in okt_pos if y in ['Noun'] if len(x) >= 2 if x not in stopwords]\n",
    "# print(okt_filtering)\n",
    "            \n",
    "# tokenizer = \" \".join(okt_filtering)\n",
    "# print(tokenizer)\n",
    "\n",
    "# #corpus = ['역사 반복 한국 일본 무역 문제 중국 반일 감정 중동 작다 분쟁 나치 독일 문제 현재 뉴스 장식 이슈 하나 하나 단지 오늘날 문제 아니다 역사 비롯 것임 역사 사람 시대 이해 있다 세계 역사 가장 다이내믹 시기 가장 중요하다 순간 장면 시기 사진 기록 최초 시기 안타깝다 흑백사진 기록 시대 저자 흑백 기억 격동 세계사 컬러 복원 역사 역사 단순하다 복원 차원 아니다 컬러풀 세계사 빛바랜 흑백 역사 우리 의미 가치 확고 분명하다 위함 사진 생각 많다 역사 가치 선명하다 잔인하다 전장 모습 희생 표정 용소 처참하다 얼굴 혁명가 찰나 순간 모든 최초 기록 컬러 마주 역사 순간 강렬하다 흥미롭다 우리 뇌리 선명하다 이미지 기초 역사 기억 놀랍다 역사 영국 베스트셀러 작가 이자 역사 가인 존스 간단명료 해설 어렵다 대사 평생 연대 워드 인물 역사 기억 있다 선명하다 사진 중심 역사 기억 역사 지금껏 없다 역사 통해 현재 있다 역사 바로 어제 일인 생명력 우리 상상 이상']\n",
    "# corpus = [tokenizer]\n",
    "# vect = CountVectorizer()\n",
    "# document_term_matrix = vect.fit_transform(corpus)# 문서-단어 행렬 \n",
    "# #print(document_term_matrix.toarray())\n",
    "\n",
    "# tf = pd.DataFrame(document_term_matrix.toarray(), columns=vect.get_feature_names())\n",
    "# #print(tf)\n",
    "#                                              # TF (Term Frequency)\n",
    "# D = len(tf)\n",
    "# df = tf.astype(bool).sum(axis=0)\n",
    "# idf = np.log((D+1) / (df+1)) + 1             # IDF (Inverse Document Frequency)\n",
    "\n",
    "# # TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "# tfidf = tf * idf                      \n",
    "# tfidf = tfidf / np.linalg.norm(tfidf, axis=1, keepdims=True)\n",
    "# tfidf = tfidf.transpose()\n",
    "# tfidf.columns = ['Number']\n",
    "# #print(tfidf)\n",
    "\n",
    "\n",
    "# s = tfidf.sort_values(by=\"Number\", ascending=False)[:5]\n",
    "# print(s)\n",
    "# print(s['Number'][0])\n",
    "\n",
    "# #tfidf.to_csv('tfidf.csv', index=False, encoding='UTF-8')\n",
    "# #s = sorted(tfidf, key = tfidf['Number'],reverse = True)[:5]\n",
    "# #print(s)\n",
    "# #print(tfidf['베스트셀러'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c169c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Build...\n",
      "(('역사', 'NNG'),)\t0.16389\n",
      "(('문제', 'NNG'),)\t0.103636\n",
      "(('기억', 'NNG'),)\t0.100006\n",
      "(('기록', 'NNG'),)\t0.0725639\n",
      "(('흑백', 'NNP'),)\t0.0716636\n",
      "(('세계사', 'NNG'),)\t0.0686902\n",
      "(('순간', 'NNG'),)\t0.0682784\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# import networkx\n",
    "# import re\n",
    " \n",
    "\n",
    "# class RawSentence:\n",
    "#     def __init__(self, textIter):\n",
    "#         if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "#         else: self.textIter = textIter\n",
    "#         self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "#     def __iter__(self):\n",
    "#         for line in self.textIter:\n",
    "#             ch = self.rgxSplitter.split(line)\n",
    "#             for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "#                 if not s: continue\n",
    "#                 yield s\n",
    " \n",
    "# class RawSentenceReader:\n",
    "#     def __init__(self, filepath):\n",
    "#         self.filepath = filepath\n",
    "#         self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "#     def __iter__(self):\n",
    "#         for line in open(self.filepath, encoding='utf-8'):\n",
    "#             ch = self.rgxSplitter.split(line)\n",
    "#             for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "#                 if not s: continue\n",
    "#                 yield s\n",
    " \n",
    "# class RawTagger:\n",
    "#     def __init__(self, textIter, tagger = None):\n",
    "#         if tagger:\n",
    "#             self.tagger = tagger\n",
    "#         else :\n",
    "#             from konlpy.tag import Komoran\n",
    "#             self.tagger = Komoran()\n",
    "#         if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "#         else: self.textIter = textIter\n",
    "#         self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "#     def __iter__(self):\n",
    "#         for line in self.textIter:\n",
    "#             ch = self.rgxSplitter.split(line)\n",
    "#             for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "#                 if not s: continue\n",
    "#                 yield self.tagger.pos(s)\n",
    " \n",
    "# class RawTaggerReader:\n",
    "#     def __init__(self, filepath, tagger = None):\n",
    "#         if tagger:\n",
    "#             self.tagger = tagger\n",
    "#         else :\n",
    "#             from konlpy.tag import Komoran\n",
    "#             self.tagger = Komoran()\n",
    "#         self.filepath = filepath\n",
    "#         self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "#     def __iter__(self):\n",
    "#         for line in open(self.filepath, encoding='utf-8'):\n",
    "#             ch = self.rgxSplitter.split(line)\n",
    "#             for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "#                 if not s: continue\n",
    "#                 yield self.tagger.pos(s)\n",
    " \n",
    "# class TextRank:\n",
    "#     def __init__(self, **kargs):\n",
    "#         self.graph = None\n",
    "#         self.window = kargs.get('window', 5)\n",
    "#         self.coef = kargs.get('coef', 1.0)\n",
    "#         self.threshold = kargs.get('threshold', 0.005)\n",
    "#         self.dictCount = {}\n",
    "#         self.dictBiCount = {}\n",
    "#         self.dictNear = {}\n",
    "#         self.nTotal = 0\n",
    " \n",
    " \n",
    "#     def load(self, sentenceIter, wordFilter = None):\n",
    "#         def insertPair(a, b):\n",
    "#             if a > b: a, b = b, a\n",
    "#             elif a == b: return\n",
    "#             self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    " \n",
    "#         def insertNearPair(a, b):\n",
    "#             self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
    " \n",
    "#         for sent in sentenceIter:\n",
    "#             for i, word in enumerate(sent):\n",
    "#                 if wordFilter and not wordFilter(word): continue\n",
    "#                 self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "#                 self.nTotal += 1\n",
    "#                 if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
    "#                 if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
    "#                 for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "#                     if wordFilter and not wordFilter(sent[j]): continue\n",
    "#                     if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "#     def loadSents(self, sentenceIter, tokenizer = None):\n",
    "#         import math\n",
    "#         def similarity(a, b):\n",
    "#             n = len(a.intersection(b))\n",
    "#             return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "#         if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "#         sentSet = []\n",
    "#         for sent in filter(None, sentenceIter):\n",
    "#             if type(sent) == str:\n",
    "#                 if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "#                 else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "#             else: s = set(sent)\n",
    "#             if len(s) < 2: continue\n",
    "#             self.dictCount[len(self.dictCount)] = sent\n",
    "#             sentSet.append(s)\n",
    " \n",
    "#         for i in range(len(self.dictCount)):\n",
    "#             for j in range(i+1, len(self.dictCount)):\n",
    "#                 s = similarity(sentSet[i], sentSet[j])\n",
    "#                 if s < self.threshold: continue\n",
    "#                 self.dictBiCount[i, j] = s\n",
    " \n",
    "#     def getPMI(self, a, b):\n",
    "#         import math\n",
    "#         co = self.dictNear.get((a, b), 0)\n",
    "#         if not co: return None\n",
    "#         return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "#     def getI(self, a):\n",
    "#         import math\n",
    "#         if a not in self.dictCount: return None\n",
    "#         return math.log(self.nTotal / self.dictCount[a])\n",
    " \n",
    "#     def build(self):\n",
    "#         self.graph = networkx.Graph()\n",
    "#         self.graph.add_nodes_from(self.dictCount.keys())\n",
    "#         for (a, b), n in self.dictBiCount.items():\n",
    "#             self.graph.add_edge(a, b, weight=n*self.coef + (1-self.coef))\n",
    " \n",
    "#     def rank(self):\n",
    "#         return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "#     def extract(self, ratio = 0.1):\n",
    "#         ranks = self.rank()\n",
    "#         cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
    "#         pairness = {}\n",
    "#         startOf = {}\n",
    "#         tuples = {}\n",
    "#         for k in cand:\n",
    "#             tuples[(k,)] = self.getI(k) * ranks[k]\n",
    "#             for l in cand:\n",
    "#                 if k == l: continue\n",
    "#                 pmi = self.getPMI(k, l)\n",
    "#                 if pmi: pairness[k, l] = pmi\n",
    " \n",
    "#         for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
    "#             print(k[0], l[0], pairness[k, l])\n",
    "#             if k not in startOf: startOf[k] = (k, l)\n",
    " \n",
    "#         for (k, l), v in pairness.items():\n",
    "#             pmis = v\n",
    "#             rs = ranks[k] * ranks[l]\n",
    "#             path = (k, l)\n",
    "#             tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    "#             last = l\n",
    "#             while last in startOf and len(path) < 7:\n",
    "#                 if last in path: break\n",
    "#                 pmis += pairness[startOf[last]]\n",
    "#                 last = startOf[last][1]\n",
    "#                 rs *= ranks[last]\n",
    "#                 path += (last,)\n",
    "#                 tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    " \n",
    "#         used = set()\n",
    "#         both = {}\n",
    "#         for k in sorted(tuples, key=tuples.get, reverse=True):\n",
    "#             if used.intersection(set(k)): continue\n",
    "#             both[k] = tuples[k]\n",
    "#             for w in k: used.add(w)\n",
    " \n",
    "#         #for k in cand:\n",
    "#         #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
    " \n",
    "#         return both\n",
    " \n",
    "#     def summarize(self, ratio = 0.333):\n",
    "#         r = self.rank()\n",
    "#         ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
    "#         return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))\n",
    "    \n",
    "\n",
    "# tr = TextRank(window=5, coef=1)\n",
    "# print('Load...')\n",
    "# stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV'), ('없', 'VV') , ('역사의 색', 'NNP')])\n",
    "# tr.load(RawTaggerReader('./text'), lambda w: w not in stopword and (w[1] in ('NNG', 'NNP','VA')))\n",
    "# print('Build...')\n",
    "# tr.build()\n",
    "# kw = tr.extract(0.1)\n",
    "# # for k in sorted(kw, key = kw.get, reverse=True):\n",
    "# #     print(\"%s\\t%g\" % (k, kw[k]))\n",
    "# for k in sorted(kw, key=kw.get, reverse=True):\n",
    "#     if len(k[0][0]) >= 2:\n",
    "#         print(\"%s\\t%g\" % (k, kw[k]))\n",
    "    \n",
    "# print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e9b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7dd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a60b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659664d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12897ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d72f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627be22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eb2ff6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0cd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b72b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Rake object at 0x7ff8aabfac10>\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import absolute_import\n",
    "# # Implementation of RAKE - Rapid Automtic Keyword Exraction algorithm\n",
    "# # as described in:\n",
    "# # Rose, S., D. Engel, N. Cramer, and W. Cowley (2010).\n",
    "# # Automatic keyword extraction from indi-vidual documents.\n",
    "# # In M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd.\n",
    "\n",
    "# import re\n",
    "# import operator\n",
    "# import io\n",
    "# from collections import defaultdict\n",
    "\n",
    "# __all__ = [\n",
    "#     'Rake',\n",
    "#     'SmartStopList',\n",
    "#     'FoxStopList',\n",
    "#     'MySQLStopList',\n",
    "#     'NLTKStopList',\n",
    "#     'GoogleSearchStopList',\n",
    "#     'RanksNLLongStopList',\n",
    "# ]\n",
    "\n",
    "\n",
    "# def is_number(s):\n",
    "#     try:\n",
    "#         float(s) if '.' in s else int(s)\n",
    "#         return True\n",
    "#     except ValueError:\n",
    "#         return False\n",
    "\n",
    "\n",
    "# def SmartStopList():\n",
    "#     from .stoplists import SmartStopList\n",
    "#     return SmartStopList.words()\n",
    "\n",
    "\n",
    "# def FoxStopList():\n",
    "#     from .stoplists import FoxStopList\n",
    "#     return FoxStopList.words()\n",
    "\n",
    "\n",
    "# def MySQLStopList():\n",
    "#     from .stoplists import MySQLStopList\n",
    "#     return MySQLStopList.words()\n",
    "\n",
    "\n",
    "# def NLTKStopList():\n",
    "#     from .stoplists import NLTKStopList\n",
    "#     return NLTKStopList.words()\n",
    "\n",
    "\n",
    "# def GoogleSearchStopList():\n",
    "#     from .stoplists import GoogleSearchStopList\n",
    "#     return GoogleSearchStopList.words()\n",
    "\n",
    "\n",
    "# def RanksNLLongStopList():\n",
    "#     from .stoplists import RanksNLLongStopList\n",
    "#     return RanksNLLongStopList.words()\n",
    "\n",
    "\n",
    "# def RanksNLStoplist():\n",
    "#     from .stoplists import RanksNLStoplist\n",
    "#     return RanksNLStoplist.words()\n",
    "\n",
    "\n",
    "# def load_stop_words(stop_word_file, regex):\n",
    "#     with io.open(stop_word_file, encoding='utf8') as stop_word_file:\n",
    "#         stop_words = re.split(regex, stop_word_file.read())\n",
    "#     return [word for word in stop_words if word not in ('', ' ')]  # filters empty string matches\n",
    "\n",
    "\n",
    "# def separate_words(text):\n",
    "#     \"\"\"\n",
    "#     Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
    "#     @param text The text that must be split in to words.\n",
    "#     @param min_word_return_size The minimum no of characters a word must have to be included.\n",
    "#     \"\"\"\n",
    "#     splitter = re.compile(r'(?u)\\W+')\n",
    "#     words = []\n",
    "#     for single_word in splitter.split(text):\n",
    "#         current_word = single_word.strip().lower()\n",
    "#         # leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
    "#         if current_word != '' and not is_number(current_word):\n",
    "#             words.append(current_word)\n",
    "#     return words\n",
    "\n",
    "\n",
    "# def split_sentences(text):\n",
    "#     \"\"\"\n",
    "#     Utility function to return a list of sentences.\n",
    "#     @param text The text that must be split in to sentences.\n",
    "#     \"\"\"\n",
    "#     sentence_delimiters = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s')\n",
    "#     sentences = sentence_delimiters.split(text)\n",
    "#     return sentences\n",
    "\n",
    "\n",
    "# def build_stop_word_regex(stop_word_list):\n",
    "#     stop_word_regex_list = []\n",
    "#     for word in stop_word_list:\n",
    "#         word_regex = r'\\b' + word + r'(?![\\w-])'\n",
    "#         stop_word_regex_list.append(word_regex)\n",
    "#     return re.compile('(?u)' + '|'.join(stop_word_regex_list), re.IGNORECASE)\n",
    "\n",
    "\n",
    "# def generate_candidate_keywords(sentence_list, stop_word_pattern, minCharacters, maxWords):\n",
    "#     phrase_list = []\n",
    "#     for s in sentence_list:\n",
    "#         tmp = re.sub(stop_word_pattern, '|', s.strip())\n",
    "#         phrases = tmp.split(\"|\")\n",
    "#         for phrase in phrases:\n",
    "#             phrase = phrase.strip().lower()\n",
    "#             if phrase != '' and len(phrase) >= minCharacters and len(phrase.split()) <= maxWords:\n",
    "#                 phrase_list.append(phrase)\n",
    "#     return phrase_list\n",
    "\n",
    "\n",
    "# def calculate_word_scores(phraseList):\n",
    "#     word_frequency = {}\n",
    "#     word_degree = {}\n",
    "#     for phrase in phraseList:\n",
    "#         word_list = separate_words(phrase)\n",
    "#         word_list_length = len(word_list)\n",
    "#         word_list_degree = word_list_length - 1\n",
    "#         for word in word_list:\n",
    "#             word_frequency.setdefault(word, 0)\n",
    "#             word_frequency[word] += 1\n",
    "#             word_degree.setdefault(word, 0)\n",
    "#             word_degree[word] += word_list_degree\n",
    "#     for item in word_frequency:\n",
    "#         word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "#     # Calculate Word scores = deg(w)/frew(w)\n",
    "#     word_score = {}\n",
    "#     for item in word_frequency:\n",
    "#         word_score.setdefault(item, 0)\n",
    "#         word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)\n",
    "#     return word_score\n",
    "\n",
    "\n",
    "# def generate_candidate_keyword_scores(phrase_list, word_score, minFrequency):\n",
    "#     keyword_candidates = {}\n",
    "#     # iterate over the list once to count how many times\n",
    "#     # each phrase occurs\n",
    "#     phrase_counts = defaultdict(int)\n",
    "#     for p in phrase_list:\n",
    "#         phrase_counts[p] += 1\n",
    "#     # and then a second time to compute RAKE scores\n",
    "#     for phrase in phrase_list:\n",
    "#         if phrase_counts[phrase] >= minFrequency:\n",
    "#             keyword_candidates.setdefault(phrase, 0)\n",
    "#             word_list = separate_words(phrase)\n",
    "#             candidate_score = 0\n",
    "#             for word in word_list:\n",
    "#                 candidate_score += word_score[word]\n",
    "#             keyword_candidates[phrase] = candidate_score\n",
    "#     return keyword_candidates\n",
    "\n",
    "\n",
    "# class Rake(object):\n",
    "\n",
    "#     def __init__(self, stop_words, regex=r'[\\W\\n]+'):\n",
    "#         #lets users call predefined stopwords easily in a platform agnostic manner or use their own list\n",
    "#         if isinstance(stop_words, list):\n",
    "#             self.__stop_words_pattern = build_stop_word_regex(stop_words)\n",
    "#         else:\n",
    "#             self.__stop_words_pattern = build_stop_word_regex(load_stop_words(stop_words, regex))\n",
    "\n",
    "#     def run(self, text, minCharacters=1, maxWords=5, minFrequency=1):\n",
    "#         sentence_list = split_sentences(text)\n",
    "\n",
    "#         phrase_list = generate_candidate_keywords(sentence_list, self.__stop_words_pattern, minCharacters, maxWords)\n",
    "\n",
    "#         word_scores = calculate_word_scores(phrase_list)\n",
    "\n",
    "#         keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores, minFrequency)\n",
    "\n",
    "#         sorted_keywords = sorted(keyword_candidates.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#         return sorted_keywords\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     corpus = ['역사는 반복된다. 한국과 일본의 무역 문제, 중국의 반일 감정, 중동의 크고 작은 분쟁, 나치 독일의 문제 등 현재 뉴스를 장식하는 이슈를 하나하나 따라가다 보면 단지 오늘날의 문제가 아니라 역사에서 비롯된 것임을 깨닫게 된다. 역사를 아는 사람만이 현 시대를 이해할 수 있다. <br/><br/>이 책은 세계의 역사에서 가장 다이내믹한 시기인 1850~1960년, 가장 중요한 순간들 200장면을 담았다. 이 시기는 사진의 기록이 남게 된 최초의 시기이나, 안타깝게도 흑백사진으로만 기록이 남은 시대다. 저자들은 흑백으로만 기억되는 격동기 세계사를 컬러로 복원하여 ‘역사의 색’을 찾고자 했다. 역사의 색을 찾는다는 것은 단순히 색을 복원하는 차원의 것이 아니다. 컬러풀한 세계사를 보여주면서 빛바랜 흑백의 역사를 되살려 우리에게 그 의미와 가치를 확고하고도 분명하게 알려주기 위함이다. 사진 한 장이 보여주는 것은 생각보다 많다. 그리고 역사가 제 빛을 찾았을 때 그 가치는 더 선명해진다. 잔인한 전장의 모습과 희생자의 표정들, 수용소의 처참한 얼굴들, 혁명가가 만나던 찰나의 순간, 그 모든 최초의 기록들. 컬러로 마주한 역사의 순간은 강렬하고 흥미롭다. 그리하여 우리에 뇌리에 선명하게 새겨진다.<br/>이 책은 이미지를 기초로 역사를 기억하게 해주는 놀라운 역사책이다. 영국 베스트셀러 작가이자 역사가인 댄 존스의 간단명료한 해설은 어렵게만 느껴졌던 현대사를 평생 내 것이 되게 만든다. 연대 순으로, 키워드 순으로, 인물 순으로 역사를 기억하는 책은 있었지만, 선명한 사진을 중심으로 역사를 기억하는 역사책은 지금껏 없었다.<br/><br/>역사를 통해 현재를 알고 싶은 역알못이 있다면, 이 책을 권한다. 색을 되찾은 역사는 바로 어제의 일인 듯 생명력을 지닌 채 우리에게 다가온다. 색의 힘은 상상 그 이상이다.']\n",
    "    \n",
    "#     print(Rake(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8884e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "7\n",
      "{'I': 0.10714285714285714, 'graduate': 0.03571428571428571, 'want': 0.03571428571428571, 'learn': 0.03571428571428571, 'Python': 0.14285714285714285, 'like': 0.03571428571428571, 'learning': 0.07142857142857142, 'easy': 0.03571428571428571, 'interesting': 0.03571428571428571, 'Learning': 0.03571428571428571, 'increases': 0.03571428571428571, 'thinking': 0.03571428571428571, 'Everyone': 0.03571428571428571, 'invest': 0.03571428571428571, 'time': 0.03571428571428571}\n",
      "{'I': 0.8472978603872037, 'graduate': 1.9459101490553132, 'want': 1.9459101490553132, 'learn': 1.9459101490553132, 'Python': 0.5596157879354227, 'like': 1.9459101490553132, 'learning': 1.252762968495368, 'easy': 1.9459101490553132, 'interesting': 1.9459101490553132, 'Learning': 1.9459101490553132, 'increases': 1.9459101490553132, 'thinking': 1.9459101490553132, 'Everyone': 1.9459101490553132, 'invest': 1.9459101490553132, 'time': 1.9459101490553132}\n",
      "{'I': 0.09078191361291467, 'graduate': 0.06949679103768976, 'want': 0.06949679103768976, 'learn': 0.06949679103768976, 'Python': 0.07994511256220323, 'like': 0.06949679103768976, 'learning': 0.08948306917824057, 'easy': 0.06949679103768976, 'interesting': 0.06949679103768976, 'Learning': 0.06949679103768976, 'increases': 0.06949679103768976, 'thinking': 0.06949679103768976, 'Everyone': 0.06949679103768976, 'invest': 0.06949679103768976, 'time': 0.06949679103768976}\n",
      "{'I': 0.09078191361291467, 'learning': 0.08948306917824057, 'Python': 0.07994511256220323, 'graduate': 0.06949679103768976, 'want': 0.06949679103768976}\n"
     ]
    }
   ],
   "source": [
    "# from nltk import tokenize\n",
    "# from operator import itemgetter\n",
    "# import math\n",
    "\n",
    "# doc = 'I am a graduate. I want to learn Python. I like learning Python. Python is easy. Python is interesting. Learning increases thinking. Everyone should invest time in learning'\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize \n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# total_words = doc.split()\n",
    "# total_word_length = len(total_words)\n",
    "# print(total_word_length)\n",
    "\n",
    "# total_sentences = tokenize.sent_tokenize(doc)\n",
    "# total_sent_len = len(total_sentences)\n",
    "# print(total_sent_len)\n",
    "\n",
    "# tf_score = {}\n",
    "# for each_word in total_words:\n",
    "#     each_word = each_word.replace('.','')\n",
    "#     if each_word not in stop_words:\n",
    "#         if each_word in tf_score:\n",
    "#             tf_score[each_word] += 1\n",
    "#         else:\n",
    "#             tf_score[each_word] = 1\n",
    "\n",
    "# # Dividing by total_word_length for each dictionary element\n",
    "# tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "# print(tf_score)\n",
    "\n",
    "# def check_sent(word, sentences): \n",
    "#     final = [all([w in x for w in word]) for x in sentences] \n",
    "#     sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "#     return int(len(sent_len))\n",
    "\n",
    "# idf_score = {}\n",
    "# for each_word in total_words:\n",
    "#     each_word = each_word.replace('.','')\n",
    "#     if each_word not in stop_words:\n",
    "#         if each_word in idf_score:\n",
    "#             idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "#         else:\n",
    "#             idf_score[each_word] = 1\n",
    "\n",
    "# # Performing a log and divide\n",
    "# idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "# print(idf_score)\n",
    "\n",
    "# tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "# print(tf_idf_score)\n",
    "\n",
    "# def get_top_n(dict_elem, n):\n",
    "#     result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "#     return result\n",
    "\n",
    "# print(get_top_n(tf_idf_score, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9669a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32f9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e77371e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['유신', '전야', '쓰나미', '일본', '막부', '결단', '막부', '존왕양이', '파의', '대표', '세력', '조슈', '본때', '대군', '일본', '앞바다', '효고', '개항', '영국', '압박', '천황', '메이', '절대', '강력하다', '외치', '막부', '로주', '파면', '개역', '당황', '린다', '놀란', '쇼군', '모치', '조정', '사표', '어지러움', '막부', '머리', '지경', '일본', '거대하다', '소용돌이', '무사하다', '있다']\n",
      "['유신', '전야', '쓰나미', '일본', '막부', '결단', '막부', '왕양', '파의', '대표', '세력', '본때', '대군', '일본', '앞바다', '개항', '영국', '압박', '천황', '메이', '절대', '양이', '막부', '파면', '개역', '당황', '모치', '조정', '사표', '막부', '머리', '일본', '거대', '소용돌이']\n",
      "['유신', '전야', '일본', '막부', '결단', '막부', '존왕양이', '대표', '세력', '조슈 번', '대군', '일본', '앞바다', '개항', '영국', '압박', '천황', '메이', '절대', '양이', '막부', '파면', '개역', '당황', '조정', '사표', '막부', '머리', '일본', '소용돌이']\n",
      "['메이', '천황', '개역', '앞바다', '영국']\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Okt\n",
    "# description = '<B>유신 전야의 쓰나미 앞에 일본 막부가 내린 결단은?</B><br/><br/>막부는 존왕양이파의 대표 세력인 조슈 번에 본때를 보이겠다며 35개 번 15만 대군을 일으킨다. 일본 앞바다 효고를 개항하라는 영국의 압박에 천황 고메이는 ‘절대 양이’를 강력히 외치고, 막부의 로주를 파면·개역하라는 당황스러운 명을 내린다. 이에 화들짝 놀란 쇼군 이에모치는 조정에 사표를 내미는데…. 이 어지러움에 막부의 머리가 터질 지경이다. 일본은 이 거대한 소용돌이에서 무사히 빠져나올 수 있을까?'\n",
    "\n",
    "# okt_pos = Okt().pos(description, norm=True, stem=True)   # 형태소 분석 ( norm : 정규화 )\n",
    "# okt_filtering = [x for x, y in okt_pos if y in ['Noun', 'Adjective'] if len(x) >= 2]\n",
    "# tokenizer = \" \".join(okt_filtering)\n",
    "\n",
    "# print(okt_filtering)\n",
    "\n",
    "# from konlpy.tag import Kkma\n",
    "# kkma = Kkma().pos(description)# 형태소 분석\n",
    "# kkma_filtering = [x for x, y in kkma if y in ['NNG'] if len(x) >= 2]\n",
    "# print(kkma_filtering)\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "\n",
    "# stopwords = '아 어제 가장 오늘날 먼저 보통 오늘날 본때 지금 파의 편의 가의 가인 마주 역사 비롯 것임 많다 일인 위함 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓'\n",
    "\n",
    "# komoran = Komoran().pos(description)\n",
    "# komoran_filtering = [x for x, y in komoran if y in ['NNG', 'NNP'] if len(x) >= 2 if x not in stopwords]\n",
    "# #print(komoran)\n",
    "# print(komoran_filtering)\n",
    "\n",
    "\n",
    "# count_dict = [(token, description.count(token)) for token in komoran_filtering ]\n",
    "# #print(count_dict)\n",
    "# ranked_words = sorted(count_dict, key=lambda x:x[1], reverse=True)[:30]\n",
    "# ranked_words = set(ranked_words)\n",
    "# #print(ranked_words)\n",
    "# keyword = [ keyword for keyword, freq in ranked_words ]\n",
    "# # my_set = set(keyword) #집합set으로 변환\n",
    "# # keyword = list(my_set) #list로 변환\n",
    "# five_keyword = keyword[:5]\n",
    "# print(five_keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4359acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['해방', '사의', '재인식', '설명', '라디오', '특강', '토대', '명의', '책임', '편집자', '이영훈', '교수', '그동안', '조사', '연구', '이해', '한국', '근대사', '시적', '설명', '가면', '재인식', '논문', '내용', '세기', '한국사', '외래', '문명', '전통', '명과', '상호작용', '나름', '형태', '정착', '과정', '관점', '기초', '대한민국', '이야기', '한국', '근현대사', '전면', '해석', '해석', '격동', '대한민국', '근현대사', '민족주의', '정서', '묶인', '얘기', '해석', '저자', '민족주의', '해체', '분별', '이기심', '본성', '인간', '개체', '단위', '주장', '조선왕조', '망하다', '원인', '식민지', '탈론', '친일파', '청산', '일본군', '위안부', '현대', '사의', '중요하다', '쟁점들', '새롭다', '간다']\n",
      "['근현대사', '현대', '해방', '해석', '교수']\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Okt\n",
    "# #description = '<B>유신 전야의 쓰나미 앞에 일본 막부가 내린 결단은?</B><br/><br/>막부는 존왕양이파의 대표 세력인 조슈 번에 본때를 보이겠다며 35개 번 15만 대군을 일으킨다. 일본 앞바다 효고를 개항하라는 영국의 압박에 천황 고메이는 ‘절대 양이’를 강력히 외치고, 막부의 로주를 파면·개역하라는 당황스러운 명을 내린다. 이에 화들짝 놀란 쇼군 이에모치는 조정에 사표를 내미는데…. 이 어지러움에 막부의 머리가 터질 지경이다. 일본은 이 거대한 소용돌이에서 무사히 빠져나올 수 있을까?'\n",
    "# description = '2006년 2월에 출간되었던 『해방전후사의 재인식』을 쉽게 풀어 설명한 EBS 라디오의 특강을 토대로 쓰여진 책. 네 명의 책임편집자 가운데 한 사람이었던 이영훈 교수가 그동안 조사하고 연구하고 이해한 한국 근대사를 통시적으로 설명해 가면서 『재인식』에 수록된 논문의 내용을 쉽게 풀어썼다. <br/><br/>20세기 한국사는 외래문명이 들어와 우리의 전통문명과 상호작용하면서 나름의 형태로 정착하는 과정이었다. 이와 같은 관점을 기초로 『대한민국 이야기』는 한국 근현대사를 전면 재해석한다. 역사는 해석이지만, 격동의 20세를 거친 대한민국 근현대사에서 우리는 \"민족주의\"라는 정서에 묶인 역사만을 얘기하고 해석해온 것이 사실이다. 저자는 우리를 옭아 맨 민족주의를 해체하고 분별력 있는 이기심을 본성으로 하는 인간 개체를 역사 서술의 단위로 삼아야 한다고 주장한다. 조선왕조가 패망한 원인, 식민지 수탈론, 친일파청산 문제, 일본군 위안부 문제 등 현대사의 중요한 문제와 쟁점들도 새로운 시각으로 풀어간다.'\n",
    "\n",
    "# stopwords = '아 어제 가장 오늘날 먼저 보통 가운데 옆에 지금 파의 편의 가의 가인 마주 역사 비롯 것임 많다 일인 위함 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓'\n",
    "\n",
    "\n",
    "# okt_pos = Okt().pos(description, norm=True, stem=True)   # 형태소 분석 ( norm : 정규화 )\n",
    "# okt_filtering = [x for x, y in okt_pos if y in ['Noun','Adjective'] if len(x) >= 2 if x not in stopwords]\n",
    "# tokenizer = \" \".join(okt_filtering)\n",
    "\n",
    "# print(okt_filtering)\n",
    "\n",
    "# count_dict = [(token, description.count(token)) for token in okt_filtering ]\n",
    "# #print(count_dict)\n",
    "# ranked_words = sorted(count_dict, key=lambda x:x[1], reverse=True)[:30]\n",
    "# ranked_words = set(ranked_words)\n",
    "# #print(ranked_words)\n",
    "# keyword = [ keyword for keyword, freq in ranked_words ]\n",
    "# # my_set = set(keyword) #집합set으로 변환\n",
    "# # keyword = list(my_set) #list로 변환\n",
    "# five_keyword = keyword[:5]\n",
    "# print(five_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ecd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c23a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description']\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# blog = pd.read_csv('2021_10_001001001.csv')\n",
    "# #print(blog)\n",
    "\n",
    "# blog_list = []\n",
    "\n",
    "# for k in blog.keys():\n",
    "#     if len(blog[k]) == 0:\n",
    "#         continue\n",
    "#     blog_list.append(k)\n",
    "    \n",
    "# print(blog_list[3:])\n",
    "# #print(blog_list)\n",
    "    \n",
    "# from konlpy.tag import Kkma\n",
    "# from konlpy.tag import Okt\n",
    "\n",
    "# kkma = Kkma()\n",
    "# okt = Okt()\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "\n",
    "# # def preprocessing(review, name):\n",
    "# #     total_review = ''\n",
    "# #     #인풋리뷰\n",
    "# #     for idx in range(len(review)):\n",
    "# #         r = review[idx]\n",
    "# #         #하나의 리뷰에서 문장 단위로 자르기\n",
    "# #         sentence = re.sub(name.split(' ')[0],'',r)\n",
    "# #         sentence = re.sub(name.split(' ')[1],'',sentence)\n",
    "# #         sentence = re.sub('\\n','',sentence)\n",
    "# #         sentence = re.sub('\\u200b','',sentence)\n",
    "# #         sentence = re.sub('\\xa0','',sentence)\n",
    "# #         sentence = re.sub('([a-zA-Z])','',sentence)\n",
    "# #         sentence = re.sub('[ㄱ-ㅎㅏ-ㅣ]+','',sentence)\n",
    "# #         sentence = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','',sentence)\n",
    "# #         if len(sentence) == 0:\n",
    "# #             continue\n",
    "# #         sentence = okt.pos(sentence, stem = True)\n",
    "# #         word = []\n",
    "# #         for i in sentence:\n",
    "# #             if not i[1] == 'Noun':\n",
    "# #                 continue\n",
    "# #             if len(i[0]) == 1:\n",
    "# #                 continue\n",
    "# #             word.append(i[0])\n",
    "# #         word = ' '.join(word)\n",
    "# #         word += '. '\n",
    "# #         total_review += word\n",
    "# #     return total_review\n",
    "\n",
    "# # from krwordrank.sentence import summarize_with_sentences\n",
    "\n",
    "# # stop = 0\n",
    "\n",
    "# # for b_idx in blog_list[3:]:\n",
    "# #     print(b_idx)\n",
    "# #     stop += 1\n",
    "# #     test = blog[b_idx]\n",
    "# #     st = ''\n",
    "# #     for r in range(len(test)):\n",
    "# #         texts = test[r]\n",
    "# #         texts = preprocessing(texts, b_idx)\n",
    "# #         st += texts\n",
    "# #     texts = st.split('. ')\n",
    "# #     try:\n",
    "# #         stopwords = {b_idx.split(' ')[0], b_idx.split(' ')[1]}\n",
    "# #         keywords, sents = summarize_with_sentences(\n",
    "# #                                                texts, \n",
    "# #                                                stopwords = stopwords,\n",
    "# #                                                num_keywords=100, \n",
    "# #                                                num_keysents=10\n",
    "# #                                                )\n",
    "# #     except ValueError:\n",
    "# #         print('key가 없습니다.')\n",
    "# #         print()\n",
    "# #         continue\n",
    "        \n",
    "# #     for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:7]:\n",
    "# #         #print('%8s:\\t%.4f' % (word, r))\n",
    "# #         print('#%s' % word)\n",
    "# #     print()\n",
    "# #     if stop == 50:\n",
    "# #         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a5a7988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사실적 근거 없이 거짓말로 쌓아 올린 샤머니즘적 세계관의, 친일은 악( 惡) 이고 반일은 선( 善) 이며 이웃 나라 중 일본만 악의 종족으로 감각하는 종족주의. 이 반일 종족주의의 기원, 형성, 확산, 맹위의 전 과정을 국민에게 고발하고 그 위험성을 경계하기 위한 역사서. <br/ ><br/> 이 책은 일본의 식민지 지배와 그 후의 한일 관계에 대한 오늘날 한국인의 기성 통념을 정면 부정한다.\n",
      "\n",
      "오늘날 대다수 한국인은 학교 교과 과정이나 여러 영화, 각종 역사 서적에서 접한 대로 “ 일본이 식민 지배 35년 간 한국인을 억압, 착취, 수탈, 학대했으며, 그럼에도 그 후 일본은 그를 반성, 사죄하지 않았다” 고 생각한다.\n",
      "\n",
      "저자는 이 통념이 사실에 근거하지 않았음을 주장한다.\n",
      "\n",
      "keywords : ['과정', '일본', '지배', '한국인', '식민지', '이웃', '형성', '확산', '후의', '경계']\n"
     ]
    }
   ],
   "source": [
    "# from newspaper import Article\n",
    "# from konlpy.tag import Kkma\n",
    "# from konlpy.tag import Okt\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.preprocessing import normalize\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# class SentenceTokenizer(object):\n",
    "#     def __init__(self):\n",
    "#         self.kkma = Kkma()\n",
    "#         self.okt = Okt()\n",
    "#         #self.stopwords = pd.read_csv('Korean_Stopwords')\n",
    "#         stopwords = '아 어제 가장 오늘날 먼저 보통 오늘날 본때 지금 파의 편의 가의 가인 마주 역사 비롯 것임 많다 일인 위함 휴 아이구 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 여 얼마간 약간 다소 좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 명 생각 시간 그녀 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓'\n",
    "#         self.stopwords = stopwords.split() \n",
    "# #     def url2sentences(self, url):\n",
    "# #         article = Article(url, language='ko')\n",
    "# #         article.download()\n",
    "# #         article.parse()\n",
    "# #         sentences = self.kkma.sentences(article.text)\n",
    "        \n",
    "# #         for idx in range(0, len(sentences)):\n",
    "# #             if len(sentences[idx]) <= 10:\n",
    "# #                 sentences[idx-1] += (' ' + sentences[idx])\n",
    "# #                 sentences[idx] = ''\n",
    "        \n",
    "# #         return sentences\n",
    "  \n",
    "#     def text2sentences(self, text):\n",
    "#         sentences = self.kkma.sentences(text)      \n",
    "#         for idx in range(0, len(sentences)):\n",
    "#             if len(sentences[idx]) <= 10:\n",
    "#                 sentences[idx-1] += (' ' + sentences[idx])\n",
    "#                 sentences[idx] = ''\n",
    "        \n",
    "#         return sentences\n",
    "\n",
    "#     def get_nouns(self, sentences):\n",
    "#         nouns = []\n",
    "#         for sentence in sentences:\n",
    "#             if sentence != '':\n",
    "#                 nouns.append(' '.join([noun for noun in self.okt.nouns(str(sentence)) \n",
    "#                                        if noun not in self.stopwords and len(noun) > 1]))\n",
    "        \n",
    "#         return nouns\n",
    " \n",
    "# class GraphMatrix(object):\n",
    "#     def __init__(self):\n",
    "#         self.tfidf = TfidfVectorizer()\n",
    "#         self.cnt_vec = CountVectorizer()\n",
    "#         self.graph_sentence = []\n",
    "        \n",
    "#     def build_sent_graph(self, sentence):\n",
    "#         tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "#         self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "#         return  self.graph_sentence\n",
    "        \n",
    "#     def build_words_graph(self, sentence):\n",
    "#         cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "#         vocab = self.cnt_vec.vocabulary_\n",
    "#         return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "# class Rank(object):\n",
    "#     def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "#         A = graph\n",
    "#         matrix_size = A.shape[0]\n",
    "#         for id in range(matrix_size):\n",
    "#             A[id, id] = 0 # diagonal 부분을 0으로 \n",
    "#             link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "#             if link_sum != 0:\n",
    "#                 A[:, id] /= link_sum\n",
    "#             A[:, id] *= -d\n",
    "#             A[id, id] = 1\n",
    "            \n",
    "#         B = (1-d) * np.ones((matrix_size, 1))\n",
    "#         ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "#         return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "\n",
    "# class TextRank(object):\n",
    "#     def __init__(self, text):\n",
    "#         self.sent_tokenize = SentenceTokenizer()\n",
    "        \n",
    "# #         if text[:5] in ('http:', 'https'):\n",
    "# #             self.sentences = self.sent_tokenize.url2sentences(text)\n",
    "# #         else:\n",
    "#         self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        \n",
    "#         self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "                    \n",
    "#         self.graph_matrix = GraphMatrix()\n",
    "#         self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "#         self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        \n",
    "#         self.rank = Rank()\n",
    "#         self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "#         self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "#         self.word_rank_idx =  self.rank.get_ranks(self.words_graph)\n",
    "#         self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "        \n",
    "#     def summarize(self, sent_num=3):\n",
    "#         summary = []\n",
    "#         index=[]\n",
    "#         for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "#             index.append(idx)\n",
    "        \n",
    "#         index.sort()\n",
    "#         for idx in index:\n",
    "#             summary.append(self.sentences[idx])\n",
    "        \n",
    "#         return summary\n",
    "        \n",
    "#     def keywords(self, word_num=10):\n",
    "#         rank = Rank()\n",
    "#         rank_idx = rank.get_ranks(self.words_graph)\n",
    "#         sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        \n",
    "#         keywords = []\n",
    "#         index=[]\n",
    "#         for idx in sorted_rank_idx[:word_num]:\n",
    "#             index.append(idx)\n",
    "            \n",
    "#         #index.sort()\n",
    "#         for idx in index:\n",
    "#             keywords.append(self.idx2word[idx])\n",
    "        \n",
    "#         return keywords\n",
    " \n",
    "\n",
    "# text = '사실적 근거 없이 거짓말로 쌓아올린 샤머니즘적 세계관의, 친일은 악(惡)이고 반일은 선(善)이며 이웃 나라 중 일본만 악의 종족으로 감각하는 종족주의. 이 반일 종족주의의 기원, 형성, 확산, 맹위의 전 과정을 국민에게 고발하고 그 위험성을 경계하기 위한 역사서.<br/><br/>이 책은 일본의 식민지 지배와 그 후의 한일 관계에 대한 오늘날 한국인의 기성 통념을 정면 부정한다. 오늘날 대다수 한국인은 학교 교과과정이나 여러 영화, 각종 역사서적에서 접한 대로 “일본이 식민 지배 35년간 한국인을 억압, 착취, 수탈, 학대했으며, 그럼에도 그 후 일본은 그를 반성, 사죄하지 않았다”고 생각한다. 저자는 이 통념이 사실에 근거하지 않았음을 주장한다.'\n",
    "# textrank = TextRank(text)\n",
    "# for row in textrank.summarize(3):\n",
    "#     print(row)\n",
    "#     print()\n",
    "# print('keywords :',textrank.keywords())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73f92fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['독자', '나태주', '대표', '트위터', '블로그']\n",
      "독자\n",
      "['독자', '나태주', '대표', '트위터', '블로그']\n",
      "독자 나태주 대표 트위터 블로그\n",
      "독\n"
     ]
    }
   ],
   "source": [
    "# # five_keyword = ['독자', '나태주', '대표', '트위터', '블로그']\n",
    "# # print(five_keyword[0])\n",
    "# five_keyword = '독자 나태주 대표 트위터 블로그'\n",
    "# string = five_keyword.split()\n",
    "# print(string)\n",
    "# print(string[0])\n",
    "# x = [string[0],string[1],string[2],string[3],string[4]]\n",
    "# print(x)\n",
    "# string = \" \".join(string)\n",
    "# print(string)\n",
    "# print(string[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0beba4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반일 종족주의 \n",
      "(27, 108)\n",
      "{0: '반일 종족주의 ', 1: '총, 균, 쇠', 2: '에로틱 조선', 3: '설민석의 무도 한국사 특강', 4: '학교에서 가르쳐주지 않는 일본사', 5: '설민석의 첫출발 한국사 연표 (보급판)', 6: '국화와 칼', 7: '대한민국 이야기', 8: '본격 한중일 세계사 6', 9: '대한민국 역사', 10: '나의 문화유산답사기 : 중국편 1 돈황과 하서주랑', 11: '사쿠라 진다', 12: '유발 하라리의 르네상스 전쟁 회고록', 13: '국화와 칼', 14: '나의 문화유산답사기 : 중국편 2 막고굴과 실크로드의 관문', 15: '부의 지도를 바꾼 회계의 세계사', 16: '역사의 색', 17: '요시다 쇼인 시대를 반역하다', 18: '오래된 미래', 19: '총, 균, 쇠', 20: '학교에서 가르쳐주지 않는 세계사', 21: '우린 너무 몰랐다', 22: '붕괴', 23: '나의 문화유산답사기 : 산사 순례', 24: '징비록', 25: '35년 1~5 세트', 26: '세계사를 바꾼 10가지 약'}\n",
      "{'반일 종족주의 ': 0, '총, 균, 쇠': 19, '에로틱 조선': 2, '설민석의 무도 한국사 특강': 3, '학교에서 가르쳐주지 않는 일본사': 4, '설민석의 첫출발 한국사 연표 (보급판)': 5, '국화와 칼': 13, '대한민국 이야기': 7, '본격 한중일 세계사 6': 8, '대한민국 역사': 9, '나의 문화유산답사기 : 중국편 1 돈황과 하서주랑': 10, '사쿠라 진다': 11, '유발 하라리의 르네상스 전쟁 회고록': 12, '나의 문화유산답사기 : 중국편 2 막고굴과 실크로드의 관문': 14, '부의 지도를 바꾼 회계의 세계사': 15, '역사의 색': 16, '요시다 쇼인 시대를 반역하다': 17, '오래된 미래': 18, '학교에서 가르쳐주지 않는 세계사': 20, '우린 너무 몰랐다': 21, '붕괴': 22, '나의 문화유산답사기 : 산사 순례': 23, '징비록': 24, '35년 1~5 세트': 25, '세계사를 바꾼 10가지 약': 26}\n",
      "4\n",
      "[('대한민국 역사', 0.16485261550519364), ('반일 종족주의 ', 0.0), ('총, 균, 쇠', 0.0), ('에로틱 조선', 0.0), ('설민석의 무도 한국사 특강', 0.0), ('설민석의 첫출발 한국사 연표 (보급판)', 0.0), ('국화와 칼', 0.0), ('대한민국 이야기', 0.0), ('본격 한중일 세계사 6', 0.0), ('나의 문화유산답사기 : 중국편 1 돈황과 하서주랑', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# data = pd.read_csv('2019_9_001001010.csv')\n",
    "# print(data['name'][0])\n",
    "\n",
    "# tfidf = TfidfVectorizer(stop_words='english')\n",
    "# tfidf_matrix = tfidf.fit_transform(data['five_keyword'])\n",
    "# print(tfidf_matrix.shape)\n",
    "\n",
    "# cosine_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# np.round(cosine_matrix, 4)\n",
    "\n",
    "# book2id = {}\n",
    "# for i, c in enumerate(data['name']): book2id[i] = c\n",
    "\n",
    "# # id와 movie title를 매핑할 dictionary를 생성해줍니다. \n",
    "# id2book = {}\n",
    "# for i, c in book2id.items(): id2book[c] = i\n",
    "    \n",
    "# print(book2id)\n",
    "# print(id2book)\n",
    "\n",
    "# # Toy Story의 id 추출 \n",
    "# idx = id2book['학교에서 가르쳐주지 않는 일본사'] # 0번 인덱스 \n",
    "# print(idx)\n",
    "# sim_scores = [(i, c) for i, c in enumerate(cosine_matrix[idx]) if i != idx] # 자기 자신을 제외한 책들의 유사도 및 인덱스를 추출 \n",
    "# sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse=True) # 유사도가 높은 순서대로 정렬 \n",
    "# sim_scores[0:10] # 상위 10개의 인덱스와 유사도를 추출 \n",
    "\n",
    "# sim_scores = [(book2id[i], score) for i, score in sim_scores[0:10]]\n",
    "# print(sim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5060aa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>five_keyword1</th>\n",
       "      <th>five_keyword2</th>\n",
       "      <th>five_keyword3</th>\n",
       "      <th>five_keyword4</th>\n",
       "      <th>five_keyword5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>꽃을 보듯 너를 본다</td>\n",
       "      <td>독자</td>\n",
       "      <td>인터넷</td>\n",
       "      <td>의견</td>\n",
       "      <td>의미</td>\n",
       "      <td>느낌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>마도조사 2</td>\n",
       "      <td>효성</td>\n",
       "      <td>소녀</td>\n",
       "      <td>의성</td>\n",
       "      <td>남망</td>\n",
       "      <td>청풍</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         name five_keyword1 five_keyword2 five_keyword3  \\\n",
       "0           0  꽃을 보듯 너를 본다            독자           인터넷            의견   \n",
       "1           1       마도조사 2            효성            소녀            의성   \n",
       "\n",
       "  five_keyword4 five_keyword5  \n",
       "0            의미            느낌  \n",
       "1            남망            청풍  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# data = pd.read_csv('2019_9_0010010465.csv')\n",
    "# data.head(2)\n",
    "# five_keyword = [data['five_keywor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07c41fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['과정', '일본', '지배', '한국인', '식민지']\n",
      "['독자', '숫자', '사람']\n"
     ]
    }
   ],
   "source": [
    "# selected = []\n",
    "# selected = ['독자','숫자', '사람']\n",
    "# a = [\"['독자', '나태주', '대표', '트위터', '블로그']\", \"['과정', '일본', '지배', '한국인', '식민지']\"]\n",
    "# print(a[1])\n",
    "# a[1] = selected\n",
    "# print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "150ec757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data1 = pd.read_csv('2019_9_001001046.csv')\n",
    "# data2 = pd.read_csv('2019_9_001001001.csv')\n",
    "# data3 = pd.read_csv('2019_9_001001025.csv')\n",
    "# data4 = pd.read_csv('2019_9_001001004.csv')\n",
    "# data5 = pd.read_csv('2019_9_001001010.csv')\n",
    "# data6 = pd.read_csv('2019_9_001001047.csv')\n",
    "\n",
    "\n",
    "\n",
    "# data = pd.concat([data1,data2,data3,data4,data5,data6], ignore_index = True)\n",
    "# #data = pd.concat([data3,data6], ignore_index = True)\n",
    "# data.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "# data.to_csv('data.csv' , encoding='UTF-8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0f57e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
